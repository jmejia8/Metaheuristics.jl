var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"tutorials/simple-tutorial/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"After reading this tutorial you'll become an expert using Metaheuristics module.","category":"page"},{"location":"tutorials/simple-tutorial/#Minimization-Problem","page":"Getting Started","title":"Minimization Problem","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Assume you want to optimize the following minimization problem:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Minimize:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"f(x) = 10D + sum_i=1^D x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"where xin -5 5^D, that is, each coordinate in x is between -5 and 5. Use D=10.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Note that the global optimum is obtained when x_i = 0 for all i. Thus, min f(x) = 0.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Objective function:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"f(x) = 10length(x) + sum( x.^2 - 10cos.(2pi*x) )","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Bounds:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"bounds = [-5ones(10) 5ones(10)]'","category":"page"},{"location":"tutorials/simple-tutorial/#Providing-Information","page":"Getting Started","title":"Providing Information","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Since the optimum is known, then we can provide this information to the optimizer.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"information = Information(f_optimum = 0.0)","category":"page"},{"location":"tutorials/simple-tutorial/#Common-Settings","page":"Getting Started","title":"Common Settings","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Usually users could require to limit the number of generation/iteration or the number of function evaluations. To do that, let's assume that the metaheuristic should evaluate at most 9000D times the objective function. Moreover, since information is provided, then we can set the desired accuracy (f(x) - f(x^*) ) to 10^-5.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"options = Options(f_calls_limit = 9000*10, f_tol = 1e-5)","category":"page"},{"location":"tutorials/simple-tutorial/#Choose-a-Metaheuristic","page":"Getting Started","title":"Choose a Metaheuristic","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Metaheuristics.jl provides different metaheuristics for optimization such as Evolutionary Centers Algorithm (ECA), Differential Evolution (DE), Particle Swarm Optimization (PSO), etc. In this tutorial we will use ECA, but you can use another algorithm following the same steps.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"The metaheuristics accept their parameters but share two common and optional settings information and options.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"algorithm = ECA(information = information, options = options)","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"warning: Consider changing the default parameters.\nChange population size for high dimensional problems.","category":"page"},{"location":"tutorials/simple-tutorial/#Optimize","page":"Getting Started","title":"Optimize","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Now, we are able to approximate the optimum. To do that it is necessary to use the optimize function as follows:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"result = optimize(f, bounds, algorithm)","category":"page"},{"location":"tutorials/simple-tutorial/#Get-the-Results","page":"Getting Started","title":"Get the Results","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Once optimize stopped, then we can get the approximate solutions.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Approximated minimum:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"fx = minimum(result)","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Approximated minimizer:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"x = minimizer(result)","category":"page"},{"location":"tutorials/simple-tutorial/#Get-Information-about-the-Resulting-Population","page":"Getting Started","title":"Get Information about the Resulting Population","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Sometimes it is useful to analyze the resulting population (for population-based metaheuristics). To do that you can use fvals to get objective function evaluation and positions to get their positions.","category":"page"},{"location":"tutorials/simple-tutorial/#Bonus","page":"Getting Started","title":"Bonus","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"We recommend you to save  your program in a function for performance purposes:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"using Metaheuristics\n\nfunction main()\n    # objective function\n    f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x) )\n    \n    # limits/bounds\n    bounds = [-5ones(10) 5ones(10)]'\n    \n    # information on the minimization problem\n    information = Information(f_optimum = 0.0)\n\n    # generic settings\n    options = Options(f_calls_limit = 9000*10, f_tol = 1e-5)\n    \n    # metaheuristic used to optimize\n    algorithm = ECA(information = information, options = options)\n\n    # start the minimization proccess\n    result = optimize(f, bounds, algorithm)\n\n    \n    fx = minimum(result)\n    x = minimizer(result)\n\n    @show fx\n    @show x\nend\n","category":"page"},{"location":"tutorials/simple-tutorial/#Summary","page":"Getting Started","title":"Summary","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Now you are able to approximate global optimum solutions using Metaheuristics.","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Please, be free to send me your PR, issue or any comment about this package for Julia.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can contribute as follows:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Implementing new Metaheuristics. See the tutorial \"Create Your Own Metaheuristic\" for creating new metaheuristics.\nImproving the documentation.\nReporting issues here.","category":"page"},{"location":"tutorials/create-metaheuristic/#Create-Your-Own-Metaheuristic","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/#Introduction","page":"Create Your Own Metaheuristic","title":"Introduction","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Firstly, you need to know what occurs when the optimize function is called.","category":"page"},{"location":"tutorials/create-metaheuristic/#Optimization-Process","page":"Create Your Own Metaheuristic","title":"Optimization Process","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Initialization: status = initialize!(status, parameters, problem, information, options) this function should initialize a State with population members according to the parameters provided.\nMain optimization loop: while status.stop == false do\nupdate population, parameters via update_state!(status, parameters, problem, information, options),\nand stop_criteria!(status, parameters, problem, information, options) will change status.stop.\nFinal Stage: When the loop in step 2 breaks, then a final function is called final_stage! for the final update of the state, e.g., delete infeasible solutions in population, get non-dominated solutions, etc. ","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Initialization:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function initialize!(\n                status, # an initiliazed State (if apply)\n                parameters::AbstractParameters,\n                problem,\n                information,\n                options,\n                args...;\n                kargs...\n        )\n\n    # initialize the stuff here\n    return State(0.0, zeros(0)) # replace this\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Optimization Process: In this step, the State is updated using the following function which is called at each iteration/generation.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function update_state!(\n        status,\n        parameters::AbstractParameters,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n)\n    # update any element in State \n    return\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Stopping Criteria: By default, the metaheuristics will stop in when either the number of function evaluations or iteration is exceeded. Also, you can stablish a different criteria via:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function stop_criteria!(status, parameters::MyMetaheuristics, problem, information, options)\n    # your stopping criteria here!\n    #...\n    status.stop = true\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Final Step:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function final_stage!(\n        status,\n        parameters::AbstractParameters,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n)\n    return\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#The-Algorithm-Parameters","page":"Create Your Own Metaheuristic","title":"The Algorithm Parameters","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Any proposed algorithm, let's say \"XYZ\", uses different parameters, then it is suggested to store them in a structure, e.g.:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"# structure with algorithm parameters\nmutable struct XYZ <: AbstractParameters\n    N::Int # population size\n    p_crossover::Float64 # crossover probability\n    p_mutation::Float64 # mutation probability\nend\n\n# a \"constructor\" \nfunction XYZ(;N = 0, p_crossover = 0.9, p_mutation = 0.1)\n    parameters = XYZ(N, p_crossover, p_mutation)\n\n    Algorithm(\n        parameters,\n        information = information,\n        options = options,\n    )\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"If you want to implement an algorithm outside of the Metaheuristics module, then include explicitly the methods you require (or use the Metaheuristics. prefix) as in Step 0, otherwise go to Step 1.","category":"page"},{"location":"tutorials/create-metaheuristic/#Implementing-a-Simple-Genetic-Algorithm","page":"Create Your Own Metaheuristic","title":"Implementing a Simple Genetic Algorithm","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"The following steps describe how to implement a simple Genetic Algorithm.","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-0","page":"Create Your Own Metaheuristic","title":"Step 0","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Including stuff from Metaheuristics we need.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"# base methods\nusing Metaheuristics\nimport Metaheuristics: initialize!, update_state!, final_stage!\nimport Metaheuristics: AbstractParameters, gen_initial_state, Algorithm, get_position\n# genetic operators\nimport Metaheuristics: SBX_crossover, polynomial_mutation!, create_solution, is_better\nimport Metaheuristics: reset_to_violated_bounds!","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-1:-The-Parameters","page":"Create Your Own Metaheuristic","title":"Step 1: The Parameters","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Due to we are creating a simple Genetic Algorithm (GA), let's define the parameters for the GA.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"# structure with algorithm parameters\nmutable struct MyGeneticAlgorithm <: AbstractParameters\n    N::Int # population size\n    p_crossover::Float64 # crossover probability\n    p_mutation::Float64 # mutation probability\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function MyGeneticAlgorithm(;N = 100,\n                            p_crossover = 0.9,\n                            p_mutation = 0.1,\n                            information = Information(),\n                            options = Options()\n    )\n    parameters = MyGeneticAlgorithm(N, p_crossover, p_mutation)\n\n    Algorithm(\n        parameters,\n        information = information,\n        options = options,\n    )\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-2:-Initialization","page":"Create Your Own Metaheuristic","title":"Step 2: Initialization","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Initialize population, parameters and settings before the optimization process begins. The most common initialization method is generating uniformly distribution random  number in provided bounds. Here, gen_initial_state for that purpose. Note that gen_initial_state require that parameters.N is defined.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function initialize!(\n        status,\n        parameters::MyGeneticAlgorithm,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    if options.iterations == 0\n        options.iterations = 500\n    end\n\n    if options.f_calls_limit == 0\n        options.f_calls_limit = options.iterations * parameters.N + 1\n    end\n\n    # gen_initial_state require that `parameters.N` is defined.\n    return gen_initial_state(problem,parameters,information,options,status)\n\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-3:-Evolve-Population","page":"Create Your Own Metaheuristic","title":"Step 3: Evolve Population","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Now, it is time to update (evolve) your population by using genetic operators: selection, crossover, mutation and environmental selection.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function update_state!(\n        status,\n        parameters::MyGeneticAlgorithm,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    population = status.population\n    N = parameters.N\n\n    for i in 1:N\n        # selection\n        parent_1 = get_position(rand(population))\n        parent_2 = get_position(rand(population))\n\n        # generate offspring  via SBX crossover\n        c,_ = SBX_crossover(parent_1, parent_2, problem.bounds, 20, parameters.p_crossover)\n\n        # Mutate solution\n        polynomial_mutation!(c, problem.bounds, 15, parameters.p_mutation)\n        # Fix solution if necessary\n        reset_to_violated_bounds!(c, problem.bounds)\n\n        # crate the solution and evaluate fitness (x, f(x))\n        offspring = create_solution(c, problem)\n\n        push!(population, offspring)\n    end\n\n    # environmental selection\n    sort!(population, lt = is_better, alg=PartialQuickSort(N))\n    deleteat!(population, N+1:length(population))\n\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-4:-After-Evolution","page":"Create Your Own Metaheuristic","title":"Step 4: After Evolution","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"This step is optional, but here is used to get the elite solution aka the best solution found by our GA.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function final_stage!(\n        status,\n        parameters::MyGeneticAlgorithm,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    # first solution is the best one since population is ordered in previous step\n    status.best_sol = status.population[1]\n    status.final_time=time()\n    return\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-5:-Time-to-Optimize","page":"Create Your Own Metaheuristic","title":"Step 5: Time to Optimize","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Now, we are able to solve and optimization problem using our genetic algorithm.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"compat: Optimization Problems\nAs you can see, MyGeneticAlgorithm was not restricted to any kind of optization problems, however works for constrained, unconstrained single- and multi-objective problems; why? The method gen_initial_state creates a State according to the output of the objective function f, whilst is_better is comparing solutions according to the solution type.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function main()\n    # test problem\n    f, bounds, _ = Metaheuristics.TestProblems.rastrigin()\n\n    # optimize and get the results\n    res = optimize(f, bounds, MyGeneticAlgorithm())\n    display(res)\nend\n\nmain()","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Output:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"+=========== RESULT ==========+\n  iteration: 500\n    minimum: 1.41152e-06\n  minimizer: [6.98505513305995e-6, -1.1651666613615994e-5, -4.343967193003195e-6, 3.567365134464557e-5, 1.3393840640183734e-5, 5.591802709915942e-5, -1.477407456986382e-5, 6.325103756718973e-6, 1.9153467328726614e-5, 4.132106648380982e-5]\n    f calls: 50000\n total time: 1.3685 s\n+============================+","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"See optimize for more information.","category":"page"},{"location":"tutorials/create-metaheuristic/#Exercises","page":"Create Your Own Metaheuristic","title":"Exercises","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Test your algorithm on a multi-objective optimization problem. Suggestion: change rastrigin by ZDT1.\nImplement an interest metaheuristic and make a PR to the Metaheuristics.jl repo on github.","category":"page"},{"location":"indicators/#Performance-Indicators","page":"Performance Indicators","title":"Performance Indicators","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Metaheuristics.jl includes performance indicators to assess evolutionary optimization algorithms performance.","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Available indicators:","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Pages = [ \"indicators.md\"]\nDepth = 3","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Performance Indicators in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"warning: Minimization is always assumed here.\nNote that in Metaheuristics.jl, minimization is always assumed. Therefore these indicators have been developed for minimization problems.","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators","text":"PerformanceIndicators\n\nThis module includes performance indicators to assess evolutionary multi-objective optimization algorithms.\n\ngd Generational Distance.\nigd Inverted Generational Distance.\ngd_plus Generational Distance plus.\nigd_plus Inverted Generational Distance plus.\ncovering Covering indicator (C-metric).\nhypervolume Hypervolume indicator.\n\nExample\n\njulia> import Metaheuristics: PerformanceIndicators, TestProblems\n\njulia> A = [ collect(1:3) collect(1:3) ]\n3×2 Array{Int64,2}:\n 1  1\n 2  2\n 3  3\n\njulia> B = A .- 1\n3×2 Array{Int64,2}:\n 0  0\n 1  1\n 2  2\n\njulia> PerformanceIndicators.gd(A, B)\n0.47140452079103173\n\njulia> f, bounds, front = TestProblems.get_problem(:ZDT1);\n\njulia> front\n                          F space\n         ┌────────────────────────────────────────┐ \n       1 │⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠈⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠈⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠈⠢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2   │⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠲⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠒⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⠢⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠢⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠢⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠢⢤⣀⠀⠀⠀⠀⠀│ \n       0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⠢⢄⣀│ \n         └────────────────────────────────────────┘ \n         0                                        1\n                            f_1\n\njulia> PerformanceIndicators.igd_plus(front, front)\n0.0\n\n\n\n\n\n","category":"module"},{"location":"indicators/#Generational-Distance","page":"Performance Indicators","title":"Generational Distance","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Generational Distance in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.gd","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.gd","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.gd","text":"gd(front, true_pareto_front; p = 1)\n\nReturns the Generational Distance.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Generational-Distance-Plus","page":"Performance Indicators","title":"Generational Distance Plus","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.gd_plus","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.gd_plus","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.gd_plus","text":"gd_plus(front, true_pareto_front; p = 1)\n\nReturns the Generational Distance Plus.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Inverted-Generational-Distance","page":"Performance Indicators","title":"Inverted Generational Distance","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Inverted Generational Distance in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.igd","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.igd","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.igd","text":"igd(front, true_pareto_front; p = 1)\n\nReturns the Inverted Generational Distance.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Inverted-Generational-Distance-Plus","page":"Performance Indicators","title":"Inverted Generational Distance Plus","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.igd_plus","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.igd_plus","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.igd_plus","text":"igd_plus(front, true_pareto_front; p = 1)\n\nReturns the Inverted Generational Distance Plus.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Spacing-Indicator","page":"Performance Indicators","title":"Spacing Indicator","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.spacing","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.spacing","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.spacing","text":"spacing(A)\n\nComputes the Schott spacing indicator. spacing(A) == 0 means that vectors in A are uniformly distributed.\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Covering-Indicator-(C-metric)","page":"Performance Indicators","title":"Covering Indicator (C-metric)","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.covering","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.covering","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.covering","text":"covering(A, B)\n\nComputes the covering indicator (percentage of vectors in B that are dominated by vectors in A) from two sets with non-dominated solutions.\n\nA and B with size (n, m) where n is number of samples and m is the vector dimension.\n\nNote that covering(A, B) == 1 means that all solutions in B are dominated by those in A. Moreover, covering(A, B) != covering(B, A) in general.\n\nIf A::State and B::State, then computes covering(A.population, B.population) after ignoring dominated solutions in each set.\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Hypervolume","page":"Performance Indicators","title":"Hypervolume","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Hypervolume Indicator in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.hypervolume","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.hypervolume","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.hypervolume","text":"hypervolume(front, reference_point)\n\nComputes the hypervolume indicator, i.e., volume between points in front and reference_point.\n\nNote that each point in front must (weakly) dominates to reference_point. Also, front is a non-dominated set.\n\nIf front::State and reference_point::Vector, then computes hypervolume(front.population, reference_point) after ignoring solutions in front that do not dominate reference_point.\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Examples","page":"Performance Indicators","title":"Examples","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Computing hypervolume indicator from vectors in a Matrix","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"import Metaheuristics.PerformanceIndicators: hypervolume\n\nf1 = collect(0:10); # objective 1\nf2 = 10 .- collect(0:10); # objective 2\n\nfront = [ f1 f2 ] \n\nreference_point = [11, 11]\n\nhv = hypervolume(front, reference_point)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Now, let's compute the hypervolume implementation in Julia from result of  NSGA3 when solving DTLZ2 test problem.","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"using Metaheuristics\nimport Metaheuristics.PerformanceIndicators: hypervolume\nimport Metaheuristics: TestProblems, get_non_dominated_solutions\n\nf, bounds, true_front = TestProblems.DTLZ2();\n\nresult = optimize(f, bounds, NSGA3());\n\napprox_front = get_non_dominated_solutions(result.population)\n\nreference_point = nadir(result.population)\n\nhv = hypervolume(approx_front, reference_point)","category":"page"},{"location":"indicators/#\\Delta_p-(Delta-p)","page":"Performance Indicators","title":"Delta_p (Delta p)","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.deltap","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.deltap","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.deltap","text":"deltap(front, true_pareto_front; p = 1)\nΔₚ(front, true_pareto_front; p = 1)\n\nReturns the averaged Hausdorff distance indicator aka Δₚ (Delta p).\n\n\"Δₚ\" can be typed as \\Delta<tab>\\_p<tab>.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Soving ZDT6 using SMS-EMOA in Julia)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Present the results using fancy plots is an important part of solving optimization problems. In this part, we use the Plots.jl package which can be installed via de Pkg prompt within Julia:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Type ] and then:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"pkg> add Plots","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Or:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"julia> import Pkg; Pkg.add(\"Plots\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Once Plots is installed on your Julia distribution, you will be able to reproduce the  following examples.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Assume you want to solve the following minimization problem.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Rastrigin Surface)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Minimize:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"f(x) = 10D + sum_i=1^D  x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"where xin-5 5^D, i.e., -5 leq x_i leq 5 for i=1ldotsD. D is the dimension number, assume D=10.","category":"page"},{"location":"visualization/#Population-Distribution","page":"Visualization","title":"Population Distribution","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Let's solve the above optimization problem and plot the resulting population (projecting two specific dimensions).","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\n# positions in matrix NxD \nX = positions(result)\n\nscatter(X[:,1], X[:,2], label=\"Population\")\n\nx = minimizer(result)\nscatter!(x[1:1], x[2:2], label=\"Best solution\")\n\n\n# (optional) save figure\nsavefig(\"final-population.png\")\n","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"If your optimization problem is scalable, then you also can plot level curves. In this case, let's assume that D=2.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 2\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\n# positions in matrix NxD \nX = positions(result)\n\nxy = range(-5, 5, length=100)\ncontour(xy, xy, (a,b) -> f([a, b]))\n\nscatter!(X[:,1], X[:,2], label=\"Population\")\n\nx = minimizer(result)\nscatter!(x[1:1], x[2:2], label=\"Best solution\")\n\n\n# (optional) save figure\nsavefig(\"final-population-contour.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Objective-Function-Values","page":"Visualization","title":"Objective Function Values","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Metaheuristics implements some methods to obtain the objective function values (fitness) from the solutions in resulting population. One of the most useful method is fvals. In this case, let's use PSO.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, PSO(options=options))\n\nf_values = fvals(result)\nplot(f_values)\n\n# (optional) save figure\nsavefig(\"fvals.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Convergence","page":"Visualization","title":"Convergence","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Sometimes, it is useful to plot the convergence plot at the end of the optimization process. To do that, it is necessary to set store_convergence = true in Options. Metaheuristics implements a method called convergence.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1, store_convergence = true)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\nf_calls, best_f_value = convergence(result)\n\nplot(xlabel=\"f calls\", ylabel=\"fitness\", title=\"Convergence\")\nplot!(f_calls, best_f_value, label=\"ECA\")\n\n# (optional) save figure\nsavefig(\"convergence.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Convergence)","category":"page"},{"location":"visualization/#Animate-convergence","page":"Visualization","title":"Animate convergence","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Also, you can plot the population and convergence in the same figure.","category":"page"},{"location":"visualization/#Single-Objective-Problem","page":"Visualization","title":"Single-Objective Problem","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1, store_convergence = true)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\nf_calls, best_f_value = convergence(result)\n\nanimation = @animate for i in 1:length(result.convergence)\n    l = @layout [a b]\n    p = plot( layout=l)\n\n    X = positions(result.convergence[i])\n    scatter!(p[1], X[:,1], X[:,2], label=\"\", xlim=(-5, 5), ylim=(-5,5), title=\"Population\")\n    x = minimizer(result.convergence[i])\n    scatter!(p[1], x[1:1], x[2:2], label=\"\")\n\n    # convergence\n    plot!(p[2], xlabel=\"Generation\", ylabel=\"fitness\", title=\"Gen: $i\")\n    plot!(p[2], 1:length(best_f_value), best_f_value, label=false)\n    plot!(p[2], 1:i, best_f_value[1:i], lw=3, label=false)\n    x = minimizer(result.convergence[i])\n    scatter!(p[2], [i], [minimum(result.convergence[i])], label=false)\nend\n\n# save in different formats\n# gif(animation, \"anim-convergence.gif\", fps=30)\nmp4(animation, \"anim-convergence.mp4\", fps=30)\n","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: )","category":"page"},{"location":"visualization/#Multi-Objective-Problem","page":"Visualization","title":"Multi-Objective Problem","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, SMS_EMOA, TestProblems, pareto_front, Options\nimport Metaheuristics.PerformanceIndicators: Δₚ\nusing Plots; gr()\n\n# get test function\nf, bounds, pf = TestProblems.ZDT6();\n\n# optimize using SMS-EMOA\nresult = optimize(f, bounds, SMS_EMOA(N=70,options=Options(iterations=500,seed=0, store_convergence=true)))\n\n# true pareto front\nB = pareto_front(pf)\n# error to the true front\nerr = [ Δₚ(r.population, pf) for r in result.convergence]\n# generate plots\na = @animate for i in 1:5:length(result.convergence)\n    A = pareto_front(result.convergence[i])\n\n    p = plot(B[:, 1], B[:,2], label=\"True Pareto Front\", lw=2,layout=(1,2), size=(850, 400))\n    scatter!(p[1], A[:, 1], A[:,2], label=\"SMS-EMOA\", markersize=4, color=:black, title=\"ZDT6\")\n    plot!(p[2], eachindex(err), err, ylabel=\"Δₚ\", legend=false)\n    plot!(p[2], 1:i, err[1:i], title=\"Generation $i\")\n    scatter!(p[2], [i], err[i:i])\nend\n\n# save animation\ngif(a, \"ZDT6.gif\", fps=20)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Soving ZDT6 using SMS-EMOA in Julia)","category":"page"},{"location":"visualization/#Pareto-Front","page":"Visualization","title":"Pareto Front","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, NSGA2, TestProblems, pareto_front, Options\nusing Plots; gr()\n\nf, bounds, solutions = TestProblems.ZDT3();\n\nresult = optimize(f, bounds, NSGA2(options=Options(seed=0)))\n\nA = pareto_front(result)\nB = pareto_front(solutions)\n\nscatter(A[:, 1], A[:,2], label=\"NSGA-II\")\nplot!(B[:, 1], B[:,2], label=\"Parento Front\", lw=2)\nsavefig(\"pareto.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Live-Plotting","page":"Visualization","title":"Live Plotting","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"The optimize function has a keyword parameter named logger that contains a function pointer. Such function will receive the State at the end of each iteration in the main optimization loop.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, NSGA2, TestProblems, pareto_front, Options, fvals\nusing Plots; gr()\n\nf, bounds, solutions = TestProblems.ZDT3();\npf = pareto_front(solutions)\n\nlogger(st) = begin\n    A = fvals(st)\n    scatter(A[:, 1], A[:,2], label=\"NSGA-II\", title=\"Gen: $(st.iteration)\")\n    plot!(pf[:, 1], pf[:,2], label=\"Parento Front\", lw=2)\n    gui()\n    sleep(0.1)\nend\n\nresult = optimize(f, bounds, NSGA2(options=Options(seed=0)), logger=logger)\n","category":"page"},{"location":"api/#API-References","page":"API References","title":"API References","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"optimize","category":"page"},{"location":"api/#Metaheuristics.optimize","page":"API References","title":"Metaheuristics.optimize","text":"  optimize(\n        f::Function, # objective function\n        bounds::AbstractMatrix,\n        method::AbstractAlgorithm = ECA();\n        logger::Function = (status) -> nothing,\n  )\n\nMinimize a n-dimensional function f with domain bounds (2×n matrix) using method = ECA() by default.\n\nExample\n\nMinimize f(x) = Σx² where x ∈ [-10, 10]³.\n\nSolution:\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> result = optimize(f, bounds)\n+=========== RESULT ==========+\n  iteration: 1429\n    minimum: 2.5354499999999998e-222\n  minimizer: [-1.5135301653303966e-111, 3.8688354844737692e-112, 3.082095708730726e-112]\n    f calls: 29989\n total time: 0.1543 s\n+============================+\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"State","category":"page"},{"location":"api/#Metaheuristics.State","page":"API References","title":"Metaheuristics.State","text":"State datatype\n\nState is used to store the current metaheuristic status. In fact, the optimize function returns a State.\n\nbest_sol Stores the best solution found so far.\npopulation is an Array{typeof(best_sol)} for population-based algorithms.\nf_calls is the number of objective functions evaluations.\ng_calls  is the number of inequality constraints evaluations.\nh_calls is the number of equality constraints evaluations.\niteration is the current iteration.\nsuccess_rate percentage of new generated solutions better that their parents. \nconvergence used save the State at each iteration.\nstart_time saves the time() before the optimization process.\nfinal_time saves the time() after the optimization process.\nstop if true, then stops the optimization process.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds)\n+=========== RESULT ==========+\n| Iter.: 1009\n| f(x) = 7.16271e-163\n| solution.x = [-7.691251412064516e-83, 1.0826961235605951e-82, -8.358428300092186e-82]\n| f calls: 21190\n| Total time: 0.2526 s\n+============================+\n\njulia> minimum(state)\n7.162710802659093e-163\n\njulia> minimizer(state)\n3-element Array{Float64,1}:\n -7.691251412064516e-83\n  1.0826961235605951e-82\n -8.358428300092186e-82\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Information","category":"page"},{"location":"api/#Metaheuristics.Information","page":"API References","title":"Metaheuristics.Information","text":"Information Structure\n\nInformation can be used to store the true optimum in order to stop a metaheuristic early.\n\nProperties:\n\nf_optimum known minimum.\nx_optimum known minimizer.\n\nIf Options is provided, then optimize will stop when |f(x) - f(x_optimum)| < Options.f_tol or ‖ x - x_optimum ‖ < Options.x_tol (euclidean distance).\n\nExample\n\nIf you want an approximation to the minimum with accuracy of 1e-3 (|f(x) - f(x*)| < 1e-3), then you may use Information.\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> information = Information(f_optimum = 0.0)\nInformation(0.0, Float64[])\n\njulia> options = Options(f_tol = 1e-3)\nOptions(0.0, 0.001, 0.0, 0.0, 1000.0, 0.0, 0.0, 0, false, true, false, :minimize)\n\njulia> state = optimize(f, bounds, ECA(information=information, options=options))\n+=========== RESULT ==========+\n| Iter.: 22\n| f(x) = 0.000650243\n| solution.x = [0.022811671589729583, 0.007052331140376011, -0.008951836265056107]\n| f calls: 474\n| Total time: 0.0106 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Options","category":"page"},{"location":"api/#Metaheuristics.Options","page":"API References","title":"Metaheuristics.Options","text":"Options(;\n    x_tol::Real = 0.0,\n    f_tol::Real = 0.0,\n    g_tol::Real = 0.0,\n    h_tol::Real = 0.0,\n    f_calls_limit::Real = 0,\n    g_calls_limit::Real = 0,\n    h_calls_limit::Real = 0,\n    time_limit::Real = Inf,\n    iterations::Int = 1,\n    store_convergence::Bool = false,\n    debug::Bool = false,\n    seed = rand(UInt)\n)\n\nOptions stores common settings for metaheuristics such as the maximum number of iterations debug options, maximum number of function evaluations, etc.\n\nMain properties:\n\nx_tol tolerance to the true minimizer if specified in Information.\nf_tol tolerance to the true minimum if specified in Information.\nf_calls_limit is the maximum number of function evaluations limit.\ntime_limit is the maximum time that optimize can spend in seconds.\niterations is the maximum number iterationn permited.\nstore_convergence if true, then push the current State in State.convergence at each generation/iteration\ndebug if true, then optimize function reports the current State (and interest information) for each iterations.\nseed non-negative integer for the random generator seed.\n\nExample\n\njulia> options = Options(f_calls_limit = 1000, debug=true, seed=1)\nOptions(0.0, 0.0, 0.0, 0.0, 1000.0, 0.0, 0.0, 0, false, true, true, :minimize, 0x0000000000000001)\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds, ECA(options=options))\n[ Info: Initializing population...\n[ Info: Starting main loop...\n+=========== RESULT ==========+\n| Iter.: 1\n| f(x) = 6.97287\n| solution.x = [-2.3628796262231875, -0.6781207370770752, -0.9642728360479853]\n| f calls: 42\n| Total time: 0.0004 s\n+============================+\n\n...\n\n[ Info: Stopped since call_limit was met.\n+=========== RESULT ==========+\n| Iter.: 47\n| f(x) = 1.56768e-08\n| solution.x = [-2.2626761322304715e-5, -9.838697194048792e-5, 7.405966506272336e-5]\n| f calls: 1000\n| Total time: 0.0313 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"convergence","category":"page"},{"location":"api/#Metaheuristics.convergence","page":"API References","title":"Metaheuristics.convergence","text":"convergence(state)\n\nget the data (touple with the number of function evaluations and fuction values) to plot the convergence graph. \n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds, ECA(options=Options(store_convergence=true)))\n+=========== RESULT ==========+\n| Iter.: 1022\n| f(x) = 7.95324e-163\n| solution.x = [-7.782044850211721e-82, 3.590044165897827e-82, -2.4665318114710003e-82]\n| f calls: 21469\n| Total time: 0.3300 s\n+============================+\n\njulia> n_fes, fxs = convergence(state);\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"minimizer","category":"page"},{"location":"api/#Metaheuristics.minimizer","page":"API References","title":"Metaheuristics.minimizer","text":"minimizer(state)\n\nReturns the approximation to the minimizer (argmin f(x)) stored in state.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"minimum(state::State)","category":"page"},{"location":"api/#Base.minimum-Tuple{State}","page":"API References","title":"Base.minimum","text":"minimum(state::Metaheuristics.State)\n\nReturns the approximation to the minimum (min f(x)) stored in state.\n\n\n\n\n\n","category":"method"},{"location":"api/#Methods-for-Solutions/Individuals","page":"API References","title":"Methods for Solutions/Individuals","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"positions","category":"page"},{"location":"api/#Metaheuristics.positions","page":"API References","title":"Metaheuristics.positions","text":"positions(state)\n\nIf state.population has N solutions, then returns a N×d Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"fvals","category":"page"},{"location":"api/#Metaheuristics.fvals","page":"API References","title":"Metaheuristics.fvals","text":"fvals(state)\n\nIf state.population has N solutions, then returns a Vector with the  objective function values from items in state.population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"nfes","category":"page"},{"location":"api/#Metaheuristics.nfes","page":"API References","title":"Metaheuristics.nfes","text":"nfes(state)\n\nget the number of function evaluations.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.create_child","category":"page"},{"location":"api/#Metaheuristics.create_child","page":"API References","title":"Metaheuristics.create_child","text":"Metaheuristics.create_child(x, fx)\n\nConstructor for a solution depending on the result of fx.\n\nExample\n\njulia> import Metaheuristics\n\njulia> Metaheuristics.create_child(rand(3), 1.0)\n| f(x) = 1\n| solution.x = [0.2700437125780806, 0.5233263210622989, 0.12871108215859772]\n\njulia> Metaheuristics.create_child(rand(3), (1.0, [2.0, 0.2], [3.0, 0.3]))\n| f(x) = 1\n| g(x) = [2.0, 0.2]\n| h(x) = [3.0, 0.3]\n| x = [0.9881102595664819, 0.4816273348099591, 0.7742585077942159]\n\njulia> Metaheuristics.create_child(rand(3), ([-1, -2.0], [2.0, 0.2], [3.0, 0.3]))\n| f(x) = [-1.0, -2.0]\n| g(x) = [2.0, 0.2]\n| h(x) = [3.0, 0.3]\n| x = [0.23983577719146854, 0.3611544510766811, 0.7998754930109109]\n\njulia> population = [ Metaheuristics.create_child(rand(2), (randn(2),  randn(2), rand(2))) for i = 1:100  ]\n                           F space\n          ┌────────────────────────────────────────┐ \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⠂⠀⠀⠀⠀⠀⡇⠈⡀⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠘⠀⡇⠀⠀⠘⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠂⠀⠂⠀⠀⢀⠠⠐⠀⡇⠄⠁⠀⠀⠀⡀⠀⢁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠂⢈⠀⠈⡇⠀⡐⠃⠀⠄⠄⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠄⢐⠠⠀⡄⠀⠀⡇⠀⠂⠈⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠉⠉⠉⠉⠋⠉⠉⠉⠉⠉⠉⠙⢉⠉⠙⠉⠉⡏⠉⠉⠩⠋⠉⠩⠉⠉⠉⡉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n   f_2    │⠀⠀⠀⠀⠀⡀⠀⠀⠀⠄⠀⠀⡀⠀⠀⠂⠀⡇⠀⠀⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⠐⡇⠠⠀⠀⠀⠈⢀⠄⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠂⠀⠄⠀⡀⠀⠂⡇⠐⠘⠈⠂⠀⠈⡀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠄⠀⠀⠀⠀⠀⠂⠀⠂⠀⠀⡇⠀⠈⢀⠐⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⠀⠀⠀⠀⢁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n       -3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          -3                                       4\n                             f_1\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"get_position","category":"page"},{"location":"api/#Metaheuristics.get_position","page":"API References","title":"Metaheuristics.get_position","text":"get_position(solution)\n\nGet the position vector.\n\n\n\n\n\nget_position(bee)\n\nGet the position vector of a bee when optimize using ABC algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"fval","category":"page"},{"location":"api/#Metaheuristics.fval","page":"API References","title":"Metaheuristics.fval","text":"fval(solution)\n\nGet the objective function value (fitness) of a solution.\n\n\n\n\n\nfval(solution)\n\nGet the fitness of a bee when optimize using ABC algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.gval","category":"page"},{"location":"api/#Metaheuristics.gval","page":"API References","title":"Metaheuristics.gval","text":"gval(solution)\n\nGet the inequality constraints of a solution.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.hval","category":"page"},{"location":"api/#Metaheuristics.hval","page":"API References","title":"Metaheuristics.hval","text":"hval(solution)\n\nGet the equality constraints of a solution.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.is_feasible","category":"page"},{"location":"api/#Metaheuristics.is_feasible","page":"API References","title":"Metaheuristics.is_feasible","text":"is_feasible(solution)\n\nReturns true if solution is feasible, otherwise returns false.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.is_better","category":"page"},{"location":"api/#Metaheuristics.is_better","page":"API References","title":"Metaheuristics.is_better","text":"is_better(A, B)\nreturn true if A is better than B in a minimization problem.\nFeasibility rules and dominated criteria are used in comparison.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.dominates","category":"page"},{"location":"api/#Metaheuristics.dominates","page":"API References","title":"Metaheuristics.dominates","text":"does A dominate B?\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.compare","category":"page"},{"location":"api/#Metaheuristics.compare","page":"API References","title":"Metaheuristics.compare","text":"compare(a, b)\ncompares whether two vectors are dominated or not.\nOutput:\n`1` if argument 1 (a) dominates argument 2 (b).\n`2` if argument 2 (b) dominates argument 1 (a).\n`3` if both arguments 1 (a) and 2 (b) are incomparable.\n`0` if both arguments 1 (a) and 2 (b) are equal.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.gen_initial_state","category":"page"},{"location":"api/#Metaheuristics.gen_initial_state","page":"API References","title":"Metaheuristics.gen_initial_state","text":"gen_initial_state(problem,parameters,information,options)\n\nGenerate an initial state, i.e., compute uniformly distributed random vectors in bounds, after that are evaluated in objective function. This method require that parameters.N is valid attribute.\n\n\n\n\n\n","category":"function"},{"location":"api/#Variation-Operators","page":"API References","title":"Variation Operators","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.SBX_crossover","category":"page"},{"location":"api/#Metaheuristics.SBX_crossover","page":"API References","title":"Metaheuristics.SBX_crossover","text":"SBX_crossover(vector1, vector2, bounds, η=15, p_variable = 0.9)\n\nSimulated binomial crossover for given two Vectors{Real}.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.ECA_operator","category":"page"},{"location":"api/#Metaheuristics.ECA_operator","page":"API References","title":"Metaheuristics.ECA_operator","text":"ECA_operator(population, K, η_max)\n\nCompute a solution using ECA variation operator, K is the number of solutions used to calculate the center of mass and η_max is the maximum stepsize.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.DE_crossover","category":"page"},{"location":"api/#Metaheuristics.DE_crossover","page":"API References","title":"Metaheuristics.DE_crossover","text":"DE_crossover(x, u, CR)\n\nBinomial crossover between x and u for Differential Evolution with probability CR, i.e., v[j] = u[j] if rand() < CR, otherwise v[j] = x[j]. Return v.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.polynomial_mutation!","category":"page"},{"location":"api/#Metaheuristics.polynomial_mutation!","page":"API References","title":"Metaheuristics.polynomial_mutation!","text":"polynomial_mutation!(vector, bounds, η=20, prob = 1 / length(vector))\n\nPolynomial Mutation applied to a vector of real numbers.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.DE_mutation","category":"page"},{"location":"api/#Metaheuristics.DE_mutation","page":"API References","title":"Metaheuristics.DE_mutation","text":"DE_mutation(population, F = 1.0, strategy = :rand1)\n\nGenerate a Vector computed from population used in Differential Evolution. Parameters: F is the stepsize, strategy can be one the following :best1, :rand2, :randToBest1 or :best2.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.MOEAD_DE_reproduction","category":"page"},{"location":"api/#Metaheuristics.MOEAD_DE_reproduction","page":"API References","title":"Metaheuristics.MOEAD_DE_reproduction","text":"MOEAD_DE_reproduction(a, b, c, F, CR, p_m, η, bounds)\n\nPerform Differential Evolution operators and polynomial mutation using three vectors a, b, c and parameters F, CR, p_m, η, i.e., stepsize, crossover and mutation probability.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.binary_tournament","category":"page"},{"location":"api/#Metaheuristics.binary_tournament","page":"API References","title":"Metaheuristics.binary_tournament","text":"binary_tournament(population)\n\nApply binary tournament to obtain a solution from from population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.GA_reproduction","category":"page"},{"location":"api/#Metaheuristics.GA_reproduction","page":"API References","title":"Metaheuristics.GA_reproduction","text":"GA_reproduction(pa::AbstractVector{T},\n                pb::AbstractVector{T},\n                bounds;\n                η_cr = 20,\n                η_m  = 15,\n                p_cr = 0.9,\n                p_m  = 0.1)\n\nCrate two solutions by applying SBX to parents pa and pb and polynomial mutation to offspring. Return two vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.GA_reproduction_half","category":"page"},{"location":"api/#Metaheuristics.GA_reproduction_half","page":"API References","title":"Metaheuristics.GA_reproduction_half","text":"GA_reproduction_half(pa::AbstractVector{T},\n                pb::AbstractVector{T},\n                bounds;\n                η_cr = 20,\n                η_m  = 15,\n                p_cr = 0.9,\n                p_m  = 0.1)\n\nSame that GA_reproduction but only returns one offspring.\n\n\n\n\n\n","category":"function"},{"location":"api/#Population","page":"API References","title":"Population","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_best","category":"page"},{"location":"api/#Metaheuristics.get_best","page":"API References","title":"Metaheuristics.get_best","text":"get_best(population)\nreturn best element in population according to the `is_better` function.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.argworst","category":"page"},{"location":"api/#Metaheuristics.argworst","page":"API References","title":"Metaheuristics.argworst","text":"argworst(population)\nreturn the index of the worst element in population\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.argbest","category":"page"},{"location":"api/#Metaheuristics.argbest","page":"API References","title":"Metaheuristics.argbest","text":"argworst(population)\nreturn the index of the worst element in population\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"nadir","category":"page"},{"location":"api/#Metaheuristics.nadir","page":"API References","title":"Metaheuristics.nadir","text":"nadir(points)\n\nComputes the nadir point from a provided array of Vectors or a population or row vectors in a Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"ideal","category":"page"},{"location":"api/#Metaheuristics.ideal","page":"API References","title":"Metaheuristics.ideal","text":"ideal(points)\n\nComputes the ideal point from a provided array of Vectors or a population or row vectors in a Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.non_dominated_sort","category":"page"},{"location":"api/#Metaheuristics.non_dominated_sort","page":"API References","title":"Metaheuristics.non_dominated_sort","text":"non_dominated_sort(population)\n\nReturn a vector of integers r containing in r[i] the rank for population[i].\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_fronts","category":"page"},{"location":"api/#Metaheuristics.get_fronts","page":"API References","title":"Metaheuristics.get_fronts","text":"get_fronts(population, computed_ranks = true)\n\nReturn each sub-front in an array. If computed_ranks == true, this method assumes that fast_non_dominated_sort!(population) has been called before.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.fast_non_dominated_sort!","category":"page"},{"location":"api/#Metaheuristics.fast_non_dominated_sort!","page":"API References","title":"Metaheuristics.fast_non_dominated_sort!","text":"fast_non_dominated_sort!(population)\n\nSort population using the fast non dominated sorting algorithm. Note that s.rank is updated for each solution s ∈ population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_non_dominated_solutions_perm","category":"page"},{"location":"api/#Metaheuristics.get_non_dominated_solutions_perm","page":"API References","title":"Metaheuristics.get_non_dominated_solutions_perm","text":"get_non_dominated_solutions_perm(population)\n\nReturn a vector of integers v such that population[v] are the non dominated solutions contained in population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_non_dominated_solutions","category":"page"},{"location":"api/#Metaheuristics.get_non_dominated_solutions","page":"API References","title":"Metaheuristics.get_non_dominated_solutions","text":"get_non_dominated_solutions(population)\n\nReturn the non dominated solutions contained in population.\n\n\n\n\n\n","category":"function"},{"location":"api/#Stopping-Criteria","page":"API References","title":"Stopping Criteria","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.diff_check","category":"page"},{"location":"api/#Metaheuristics.diff_check","page":"API References","title":"Metaheuristics.diff_check","text":"diff_check(status, information, options; d = options.f_tol, p = 0.5)\n\nCheck the difference between best and worst objective function values in current population (where at least %p of solution are feasible). Return true when such difference is <= d, otherwise return false.\n\nRef. Zielinski, K., & Laur, R. (n.d.). Stopping Criteria for Differential Evolution in Constrained Single-Objective Optimization. Studies in Computational Intelligence, 111–138. doi:10.1007/978-3-540-68830-34 (https://doi.org/10.1007/978-3-540-68830-34)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.call_limit_stop_check","category":"page"},{"location":"api/#Metaheuristics.call_limit_stop_check","page":"API References","title":"Metaheuristics.call_limit_stop_check","text":"call_limit_stop_check(status, information, options)\n\nLimit the number of function evaluations, i.e.,  return status.f_calls >= options.f_calls_limit.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.iteration_stop_check","category":"page"},{"location":"api/#Metaheuristics.iteration_stop_check","page":"API References","title":"Metaheuristics.iteration_stop_check","text":"iteration_stop_check(status, information, options)\n\nUsed to limit the number of iterations.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.time_stop_check","category":"page"},{"location":"api/#Metaheuristics.time_stop_check","page":"API References","title":"Metaheuristics.time_stop_check","text":"time_stop_check(status, information, options)\n\nUsed to limit the time (in seconds), i.e., status.overall_time >= options.time_limit.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.accuracy_stop_check","category":"page"},{"location":"api/#Metaheuristics.accuracy_stop_check","page":"API References","title":"Metaheuristics.accuracy_stop_check","text":"accuracy_stop_check(status, information, options)\n\nIf the optimum is provided, then check if the accuracy is met via abs(status.best_sol.f - information.f_optimum) < options.f_tol.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.var_stop_check","category":"page"},{"location":"api/#Metaheuristics.var_stop_check","page":"API References","title":"Metaheuristics.var_stop_check","text":"var_stop_check(status, information, options)\n\nCheck if the variance is close to zero in objective space.\n\n\n\n\n\n","category":"function"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"List of implemented metaheuristics. The algorithms were implemented based on the contributor's understanding of the algorithms detailed in the published paper.","category":"page"},{"location":"algorithms/#Evolutionary-Centers-Algorithm","page":"Algorithms","title":"Evolutionary Centers Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ECA was proposed for solving global optimization problems. See J.-A. Mejía-de-Dios, E. Mezura-Montes (2019) for more information.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ECA","category":"page"},{"location":"algorithms/#Metaheuristics.ECA","page":"Algorithms","title":"Metaheuristics.ECA","text":"ECA(;\n    η_max = 2.0,\n    K = 7,\n    N = 0,\n    N_init = N,\n    p_exploit = 0.95,\n    p_bin = 0.02,\n    p_cr = Float64[],\n    adaptive = false,\n    resize_population = false,\n    information = Information(),\n    options = Options()\n)\n\nParameters for the metaheuristic ECA: step-size η_max,K is number of vectors to generate the center of mass, N is the population size.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ECA())\n+=========== RESULT ==========+\n  iteration: 1429\n    minimum: 3.3152400000000004e-223\n  minimizer: [4.213750597785841e-113, 5.290977430907081e-112, 2.231685329262638e-112]\n    f calls: 29989\n total time: 0.1672 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ECA(N = 10, η_max = 1.0, K = 3))\n+=========== RESULT ==========+\n  iteration: 3000\n    minimum: 0.000571319\n  minimizer: [-0.00017150889316537758, -0.007955828028420616, 0.022538733289139145]\n    f calls: 30000\n total time: 0.1334 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Differential-Evolution","page":"Algorithms","title":"Differential Evolution","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"DE is an evolutionary algorithm based on vector differences. See K. V. Price (2013) for more details.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"DE","category":"page"},{"location":"algorithms/#Metaheuristics.DE","page":"Algorithms","title":"Metaheuristics.DE","text":"DE(;\n    N  = 0,\n    F  = 1.0,\n    CR = 0.9,\n    strategy = :rand1,\n    information = Information(),\n    options = Options()\n)\n\nParameters for Differential Evolution (DE) algorithm: step-size F,CR controlls the binomial crossover, N is the population size. The parameter trategy is related to the variation operator (:rand1, :rand2, :best1, :best2, :randToBest1).\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], DE())\n+=========== RESULT ==========+\n  iteration: 1000\n    minimum: 0\n  minimizer: [0.0, 0.0, 0.0]\n    f calls: 30000\n total time: 0.0437 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], DE(N=50, F=1.5, CR=0.8))\n+=========== RESULT ==========+\n  iteration: 600\n    minimum: 8.68798e-25\n  minimizer: [3.2777877981303293e-13, 3.7650459509488005e-13, -7.871487597385812e-13]\n    f calls: 30000\n total time: 0.0319 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Particle-Swarm-Optimization","page":"Algorithms","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"PSO is a population-based optimization technique inspired by the motion of bird flocks and schooling fish (J. Kennedy, R. Eberhart (1995)).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"PSO","category":"page"},{"location":"algorithms/#Metaheuristics.PSO","page":"Algorithms","title":"Metaheuristics.PSO","text":"PSO(;\n    N  = 0,\n    C1 = 2.0,\n    C2 = 2.0,\n    ω  = 0.8,\n    information = Information(),\n    options = Options()\n)\n\nParameters for Particle Swarm Optimization (PSO) algorithm: learning rates C1 and C2, N is the population size and ω controls the inertia weight. \n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], PSO())\n+=========== RESULT ==========+\n  iteration: 1000\n    minimum: 1.40522e-49\n  minimizer: [3.0325415595139883e-25, 1.9862212295897505e-25, 9.543772256546461e-26]\n    f calls: 30000\n total time: 0.1558 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], PSO(N = 100, C1=1.5, C2=1.5, ω = 0.7))\n+=========== RESULT ==========+\n  iteration: 300\n    minimum: 2.46164e-39\n  minimizer: [-3.055334698085433e-20, -8.666986835846171e-21, -3.8118413472544027e-20]\n    f calls: 30000\n total time: 0.1365 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Artificial-Bee-Colony","page":"Algorithms","title":"Artificial Bee Colony","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm by D. Karaboga, B. Basturk (2007).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ABC","category":"page"},{"location":"algorithms/#Metaheuristics.ABC","page":"Algorithms","title":"Metaheuristics.ABC","text":"ABC(;\n    N = 50,\n    Ne = div(N+1, 2),\n    No = div(N+1, 2),\n    limit=10,\n    information = Information(),\n    options = Options()\n)\n\nABC implements the original parameters for the Artificial Bee Colony Algorithm. N is the population size, Ne is the number of employees, No is the number of outlookers bees. limit is related to the times that a solution is visited.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ABC())\n+=========== RESULT ==========+\n  iteration: 595\n    minimum: 4.03152e-28\n  minimizer: [1.489845115451046e-14, 1.2207275971717747e-14, -5.671872444705246e-15]\n    f calls: 30020\n total time: 0.0360 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ABC(N = 80,  No = 20, Ne = 50, limit=5))\n+=========== RESULT ==========+\n  iteration: 407\n    minimum: 8.94719e-08\n  minimizer: [8.257485723496422e-5, 0.0002852795196258074, -3.5620824723352315e-5]\n    f calls: 30039\n total time: 0.0432 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#MOEA/D-DE","page":"Algorithms","title":"MOEA/D-DE","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Multiobjective optimization problems with complicated Pareto sets by H. Li, Q. Zhang (2008).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"MOEAD_DE","category":"page"},{"location":"algorithms/#Metaheuristics.MOEAD_DE","page":"Algorithms","title":"Metaheuristics.MOEAD_DE","text":"MOEAD_DE(weights ;\n    F = 0.5,\n    CR = 1.0,\n    λ = Array{Vector{Float64}}[], # ref. points\n    η = 20,\n    p_m = -1.0,\n    T = round(Int, 0.2*length(weights)),\n    δ = 0.9,\n    n_r = round(Int, 0.02*length(weights)),\n    z = zeros(0),\n    B = Array{Int}[],\n    s1 = 0.01,\n    s2 = 20.0,\n    information = Information(),\n    options = Options())\n\nMOEAD_DE implements the original version of MOEA/D-DE. It uses the contraint handling method based on the sum of violations (for constrained optimizaton): g(x, λ, z) = max(λ .* abs.(fx - z)) + sum(max.(0, gx)) + sum(abs.(hx))\n\nTo use MOEAD_DE, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are the equality and inequality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nRef. Multiobjective Optimization Problems With Complicated Pareto Sets, MOEA/D and NSGA-II; Hui Li and Qingfu Zhang.\n\nExample\n\nAssume you want to solve the following optimizaton problem:\n\nMinimize:\n\nf(x) = (x_1, x_2)\n\nsubject to:\n\ng(x) = x_1^2 + x_2^2 - 1 ≤ 0\n\nx_1, x_2 ∈ [-1, 1]\n\nA solution can be:\n\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] )\n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\nnobjectives = 2\nnpartitions = 100\n\n# reference points (Das and Dennis's method)\nweights = gen_ref_dirs(nobjectives, npartitions)\n\n# define the parameters\nmoead_de = MOEAD_DE(weights, options=Options(debug=false, iterations = 250))\n\n# optimize\nstatus_moead = optimize(f, bounds, moead_de)\n\n# show results\ndisplay(status_moead)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Gravitational-Search-Algorithm","page":"Algorithms","title":"Gravitational Search Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Chaotic gravitational constants for the gravitational search algorithm by S. Mirjalili, A. H. Gandomi (2017)","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"CGSA","category":"page"},{"location":"algorithms/#Metaheuristics.CGSA","page":"Algorithms","title":"Metaheuristics.CGSA","text":"CGSA(;\n    N::Int    = 30,\n    chValueInitial::Real   = 20,\n    chaosIndex::Real   = 9,\n    ElitistCheck::Int    = 1,\n    Rpower::Int    = 1,\n    Rnorm::Int    = 2,\n    wMax::Real   = chValueInitial,\n    wMin::Real   = 1e-10,\n    information = Information(),\n    options = Options()\n)\n\nCGSA is an extension of the GSA algorithm but with Chaotic gravitational constants for the gravitational search algorithm.\n\nRef. Chaotic gravitational constants for the gravitational search algorithm. Applied Soft Computing 53 (2017): 407-419.\n\nParameters:\n\nN: Population size\nchValueInitial: Initial value for the chaos value\nchaosIndex: Integer 1 ≤ chaosIndex ≤ 10 is the function that model the chaos\nRpower: power related to the distance norm(x)^Rpower\nRnorm: is the value as in norm(x, Rnorm)\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], CGSA())\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 8.63808e-08\n  minimizer: [0.0002658098418993323, -1.140808975532608e-5, -0.00012488307670533095]\n    f calls: 15000\n total time: 0.1556 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], CGSA(N = 80, chaosIndex = 1))\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 1.0153e-09\n  minimizer: [-8.8507563788141e-6, -1.3050111801923072e-5, 2.7688577445980026e-5]\n    f calls: 40000\n total time: 1.0323 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Simulated-Annealing","page":"Algorithms","title":"Simulated Annealing","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Physics inspired algorithm for optimization Peter J.M. Van L., E.H.L. Aarts (1987).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SA","category":"page"},{"location":"algorithms/#Metaheuristics.SA","page":"Algorithms","title":"Metaheuristics.SA","text":"    SA(;\n        x_initial::Vector = zeros(0),\n        N::Int = 500,\n        tol_fun::Real= 1e-4,\n        information = Information(),\n        options = Options()\n    )\n\nParameters for the method of Simulated Annealing (Kirkpatrick et al., 1983).\n\nParameters:\n\nx_intial: Inital solution. If empty, then SA will generate a random one within the bounds.\nN: The number of test points per iteration.\ntol_fun: tolerance value for the Metropolis condition to accept or reject the test point as current point.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], SA())\n+=========== RESULT ==========+\n  iteration: 60\n    minimum: 5.0787e-68\n  minimizer: [-2.2522059499734615e-34, 3.816133503985569e-36, 6.934348004465088e-36]\n    f calls: 29002\n total time: 0.0943 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], SA(N = 100, x_initial = [1, 0.5, -1]))\n+=========== RESULT ==========+\n  iteration: 300\n    minimum: 1.99651e-69\n  minimizer: [4.4638292404181215e-35, -1.738939846089388e-36, -9.542441152683457e-37]\n    f calls: 29802\n total time: 0.0965 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Whale-Optimization-Algorithm","page":"Algorithms","title":"Whale Optimization Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The Whale Optimization Algorithm inspired by humpback whales S. Mirjalili, A. Lewis (2016).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"WOA","category":"page"},{"location":"algorithms/#Metaheuristics.WOA","page":"Algorithms","title":"Metaheuristics.WOA","text":"WOA(;N = 30, information = Information(), options = Options())\n\nParameters for the Whale Optimization Algorithm. N is the population size (number of whales).\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], WOA())\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 3.9154600000000003e-100\n  minimizer: [8.96670478694908e-52, -1.9291317455298046e-50, 4.3113080446722046e-51]\n    f calls: 15000\n total time: 0.0134 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], WOA(N = 100))\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 1.41908e-145\n  minimizer: [9.236161414012512e-74, -3.634919950380001e-73, 3.536831799149254e-74]\n    f calls: 50000\n total time: 0.0588 s\n+============================+\n\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NSGA-II","page":"Algorithms","title":"NSGA-II","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A fast and elitist multiobjective genetic algorithm: NSGA-IIK. Deb, A. Pratap, S. Agarwal, T. Meyarivan (2002).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"NSGA2","category":"page"},{"location":"algorithms/#Metaheuristics.NSGA2","page":"Algorithms","title":"Metaheuristics.NSGA2","text":"NSGA2(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-II.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\n\nTo use NSGA2, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `NSGA2()` for using default parameters)\nnsga2 = NSGA2(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, nsga2)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NSGA-III","page":"Algorithms","title":"NSGA-III","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints K. Deb, H. Jain (2014).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"NSGA3","category":"page"},{"location":"algorithms/#Metaheuristics.NSGA3","page":"Algorithms","title":"Metaheuristics.NSGA3","text":"NSGA3(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    partitions = 12,\n    reference_points = Vector{Float64}[],\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-III.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\nreference_points reference points usually generated by gen_ref_dirs.\npartitions number of Das and Dennis's reference points if reference_points is empty.\n\nTo use NSGA3, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n\n# Objective function, bounds, and the True Pareto front\nf, bounds, pf = Metaheuristics.TestProblems.get_problem(:DTLZ2)\n\n\n# define the parameters (use `NSGA3()` for using default parameters)\nnsga3 = NSGA3(p_cr = 0.9)\n\n# optimize\nstatus = optimize(f, bounds, nsga3)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#SMS-EMOA","page":"Algorithms","title":"SMS-EMOA","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"An EMO algorithm using the hypervolume measure as selection criterion Michael Emmerich, Nicola Beume, Boris Naujoks (2005).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SMS_EMOA","category":"page"},{"location":"algorithms/#Metaheuristics.SMS_EMOA","page":"Algorithms","title":"Metaheuristics.SMS_EMOA","text":"SMS_EMOA(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    n_samples = 10_000,\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic SMS-EMOA.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\nn_samples number of samples to approximate hypervolume in many-objective (M > 2).\n\nTo use SMS_EMOA, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `SMS_EMOA()` for using default parameters)\nsms_emoa = SMS_EMOA(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, sms_emoa)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#SPEA2","page":"Algorithms","title":"SPEA2","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Improved strength Pareto evolutionary algorithm Eckart Zitzler, Marco Laumanns, Lothar Thiele (2001).","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SPEA2","category":"page"},{"location":"algorithms/#Metaheuristics.SPEA2","page":"Algorithms","title":"Metaheuristics.SPEA2","text":"SPEA2(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-II.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\n\nTo use SPEA2, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `SPEA2()` for using default parameters)\nnsga2 = SPEA2(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, nsga2)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"problems/#Problems","page":"Problems","title":"Problems","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":"Benchmark Test Problems for numerical optimization.","category":"page"},{"location":"problems/","page":"Problems","title":"Problems","text":" Metaheuristics.TestProblems.get_problem","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.get_problem","page":"Problems","title":"Metaheuristics.TestProblems.get_problem","text":"get_problem(problem)\n\nReturns a 3-tuple with the objective function, the bounds and 100 Pareto solutions for multi-objective optimization problems or the optimal solutions for (box)constrained optimization problems.\n\nHere, problem can be one of the following symbols:\n\nSingle-objective:\n\n:sphere\n:discus\n:rastrigin\n\nMulti-objective:\n\n:ZDT1\n:ZDT2\n:ZDT3\n:ZDT4\n:ZDT6\n\nMany-objective:\n\n:DTLZ1\n:DTLZ2\n:DTLZ3\n:DTLZ4\n:DTLZ5\n:DTLZ6\n\nExample\n\njulia> import Metaheuristics: TestProblems, optimize\n\njulia> f, bounds, pareto_solutions = TestProblems.get_problem(:ZDT3);\n\n\njulia> bounds\n2×30 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\njulia> pareto_solutions\n                           F space\n          ┌────────────────────────────────────────┐ \n        1 │⢅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠈⢢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠙⠒⠀⠀⠀⠀⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠘⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠱⠄⠀⠀⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠬⡦⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠂⠀⠀⠀⠀⠀⠀⢢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢧⡀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡆⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠀⠀│ \n       -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          0                                      0.9\n                             f_1\n\n\n\n\n\n\n","category":"function"},{"location":"problems/#Box-constrained-Optimization","page":"Problems","title":"Box-constrained Optimization","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":" Metaheuristics.TestProblems.sphere","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.sphere","page":"Problems","title":"Metaheuristics.TestProblems.sphere","text":"sphere(D)\n\nThe well-known D-dimensional Sphere function.\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.discus","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.discus","page":"Problems","title":"Metaheuristics.TestProblems.discus","text":"discus(D)\n\nThe well-known D-dimensional Discus function.\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.rastrigin","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.rastrigin","page":"Problems","title":"Metaheuristics.TestProblems.rastrigin","text":"rastrigin(D)\n\nThe well-known D-dimensional Rastrigin function.\n\n\n\n\n\n","category":"function"},{"location":"problems/#Constrained-Optimization","page":"Problems","title":"Constrained Optimization","text":"","category":"section"},{"location":"problems/#Multi-objective-Optimization","page":"Problems","title":"Multi-objective Optimization","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT1","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT1","page":"Problems","title":"Metaheuristics.TestProblems.ZDT1","text":"ZDT1(D, n_solutions)\n\nZDT1 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT2","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT2","page":"Problems","title":"Metaheuristics.TestProblems.ZDT2","text":"ZDT2(D, n_solutions)\n\nZDT2 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nnonconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT3","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT3","page":"Problems","title":"Metaheuristics.TestProblems.ZDT3","text":"ZDT3(D, n_solutions)\n\nZDT3 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nconvex disconected\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT4","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT4","page":"Problems","title":"Metaheuristics.TestProblems.ZDT4","text":"ZDT4(D, n_solutions)\n\nZDT4 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nnonconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT6","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT6","page":"Problems","title":"Metaheuristics.TestProblems.ZDT6","text":"ZDT6(D, n_solutions)\n\nZDT6 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of Pareto solutions.\n\nMain properties:\n\nnonconvex\nnon-uniformly spaced\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.DTLZ2","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.DTLZ2","page":"Problems","title":"Metaheuristics.TestProblems.DTLZ2","text":"DTLZ2(m = 3, ref_dirs = gen_ref_dirs(m, 12))\n\nDTLZ2 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nm number of objective functions\nref_dirs number of Pareto solutions (default: Das and Dennis' method).\n\nMain properties:\n\nnonconvex\nunifrontal\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This package provides different tools for optimization. Hence, this section gives different examples for using the implemented Metaheuristics.","category":"page"},{"location":"examples/#Single-Objective-Optimization","page":"Examples","title":"Single-Objective Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\n\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x) ) # objective function\n\nbounds = [-5ones(10) 5ones(10)]' # limits/bounds\n\ninformation = Information(f_optimum = 0.0); # information on the minimization problem\n\noptions = Options(f_calls_limit = 9000*10, f_tol = 1e-5); # generic settings\n\nalgorithm = ECA(information = information, options = options) # metaheuristic used to optimize\n\nresult = optimize(f, bounds, algorithm) # start the minimization proccess\n\n\nminimum(result)\nminimizer(result)\n\n\nresult = optimize(f, bounds, algorithm) # note that second run is faster\n","category":"page"},{"location":"examples/#Providing-Initial-Solutions","page":"Examples","title":"Providing Initial Solutions","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Sometimes you may need to use the starter solutions you need before the optimization process begins, well, this example illustrates how to do it.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\nf, bounds, optimums = Metaheuristics.TestProblems.get_problem(:sphere);\nD = size(bounds,2);\n\nx_known = 0.6ones(D) # known solution\n\nX = [ bounds[1,:] + rand(D).* ( bounds[2,:] -  bounds[1,:]) for i in 1:19  ]; # random solutions (uniform distribution)\n\npush!(X, x_known); # save an interest solution\n\npopulation = [ Metaheuristics.create_child(x, f(x)) for x in X ]; # generate the population with 19+1 solutions\n\nprev_status = State(Metaheuristics.get_best(population), population); # prior state\n\nmethod = ECA(N = length(population))\nmethod.status = prev_status; # say to ECA that you have generated a population\n\noptimize(f, bounds, method) # optimize","category":"page"},{"location":"examples/#Constrained-Optimization","page":"Examples","title":"Constrained Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"It is common that optimization models include constraints that must be satisfied for example: The Rosenbrock function constrained to a disk","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Minimize:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"displaystyle f(xy)=(1-x)^2+100(y-x^2)^2","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"subject to:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"displaystyle x^2+y^2leq 2","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"where -2 leq xy leq 2.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"In Metaheuristics.jl, a feasible solution is such that g(x) leq 0 and h(x) approx 0. Hence, in this example the constraint is given by g(x) = x^2 + y^2 - 2 leq 0. Moreover, the equality and inequality constraints must be saved into  Arrays.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"compat: Constriants handling\nIn this package, if the algorithm was not designed for constrained optimization, then solutions with lower constraint violation sum will be preferred.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\n\nfunction f(x)\n    x,y = x[1], x[2]\n\n    fx = (1-x)^2+100(y-x^2)^2\n    gx = [x^2 + y^2 - 2] # inequality constraints\n    hx = [0.0] # equality constraints\n\n    # order is important\n    return fx, gx, hx\nend\n\nbounds = [-2.0 -2; 2 2]\n\noptimize(f, bounds, ECA(N=30, K=3))","category":"page"},{"location":"examples/#Multiobjective-Optimization","page":"Examples","title":"Multiobjective Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"To implement a multiobjective optimization problem and solve it, you can proceed as usual. Here, you need to provide constraints if they exist, otherwise put gx = [0.0]; hx = [0.0]; to indicate an unconstrained multiobjective problem.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\n\nfunction f(x)\n    # objective functions\n    v = 1.0 + sum(x .^ 2)\n    fx1 = x[1] * v\n    fx2 = (1 - sqrt(x[1])) * v\n\n    fx = [fx1, fx2]\n\n    # constraints\n    gx = [0.0] # inequality constraints\n    hx = [0.0] # equality constraints\n\n    # order is important\n    return fx, gx, hx\nend\n\nbounds = [zeros(30) ones(30)]';\n\noptimize(f, bounds, NSGA2())","category":"page"},{"location":"examples/#Modifying-an-Existing-Metaheuristic","page":"Examples","title":"Modifying an Existing Metaheuristic","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"You may need to modify one of the implemented metaheuristic to improve the algorithm performance or test new mechanisms. This example illustrate how to do it.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"warning: Modifying algorithms could break stuff\nBe cautious when modify a metaheuristic due to those changes will overwrite the default method for that metaheuristic.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's assume that we want to modify the stop criteria for ECA. See Contributing  for more details.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\nimport LinearAlgebra: norm\n\n# overwrite method\nfunction Metaheuristics.stop_criteria!(\n        status,\n        parameters::ECA, # It is important indicate the modified Metaheuristic \n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    if status.stop\n        # nothing to do\n        return\n    end\n\n    # Diversity-based stop criteria\n\n    x_mean = zeros(length(status.population[1].x))\n    for sol in status.population\n        x_mean += sol.x\n    end\n    x_mean /= length(status.population)\n    \n    distances_mean = sum(sol -> norm( x_mean - sol.x ), status.population)\n    distances_mean /= length(status.population)\n\n    # stop when solutions are close enough to the geometrical center\n    new_stop_condition = distances_mean <= 1e-3\n\n    status.stop = new_stop_condition\n\n    # (optional and not recommended) print when this critaria is met\n    if status.stop\n        @info \"Diversity-based stop criteria\"\n        @show distances_mean\n    end\n\n\n    return\nend\n\nf, bounds, opt = Metaheuristics.TestProblems.get_problem(:sphere);\noptimize(f, bounds, ECA())\n","category":"page"},{"location":"#Metaheuristics-an-Intuitive-Package-for-Global-Optimization","page":"Index","title":"Metaheuristics - an Intuitive Package for Global Optimization","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Author: Jesus Mejía (@jmejia8)","category":"page"},{"location":"","page":"Index","title":"Index","text":"High performance algorithms for optimization purely coded in a high performance language.","category":"page"},{"location":"","page":"Index","title":"Index","text":"(Image: Source) (Image: Build Status) (Image: codecov) (Image: DOI)","category":"page"},{"location":"#Introduction","page":"Index","title":"Introduction","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Optimization is one of the most common task in the scientific and industry field but real-world problems require high-performance algorithms to optimize non-differentiable, non-convex, dicontinuous functions. Different metaheuristics algorithms have been proposed to solve optimization problems but without strong assumptions about the objective function.","category":"page"},{"location":"","page":"Index","title":"Index","text":"This package implements state-of-the-art metaheuristics algorithms for global optimization. The aim of this package is to provide easy to use (and fast) metaheuristics for numerical global optimization.","category":"page"},{"location":"#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Open the Julia (Julia 1.1 or Later) REPL and press ] to open the Pkg prompt. To add this package, use the add command:","category":"page"},{"location":"","page":"Index","title":"Index","text":"pkg> add Metaheuristics","category":"page"},{"location":"","page":"Index","title":"Index","text":"Or, equivalently, via the Pkg API:","category":"page"},{"location":"","page":"Index","title":"Index","text":"julia> import Pkg; Pkg.add(\"Metaheuristics\")","category":"page"},{"location":"#Quick-Start","page":"Index","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Assume you want to solve the following minimization problem.","category":"page"},{"location":"","page":"Index","title":"Index","text":"(Image: Rastrigin Surface)","category":"page"},{"location":"","page":"Index","title":"Index","text":"Minimize:","category":"page"},{"location":"","page":"Index","title":"Index","text":"f(x) = 10D + sum_i=1^D  x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"","page":"Index","title":"Index","text":"where xin-5 5^D, i.e., -5 leq x_i leq 5 for i=1ldotsD. D is the dimension number, assume D=10.","category":"page"},{"location":"#Solution","page":"Index","title":"Solution","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Firstly, import the Metaheuristics package:","category":"page"},{"location":"","page":"Index","title":"Index","text":"using Metaheuristics","category":"page"},{"location":"","page":"Index","title":"Index","text":"Code the objective function:","category":"page"},{"location":"","page":"Index","title":"Index","text":"f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )","category":"page"},{"location":"","page":"Index","title":"Index","text":"Instantiate the bounds, note that bounds should be a 2times 10 Matrix where the first row corresponds to the lower bounds whilst the second row corresponds to the upper bounds.","category":"page"},{"location":"","page":"Index","title":"Index","text":"D = 10\nbounds = [-5ones(D) 5ones(D)]'","category":"page"},{"location":"","page":"Index","title":"Index","text":"Approximate the optimum using the function optimize.","category":"page"},{"location":"","page":"Index","title":"Index","text":"result = optimize(f, bounds)","category":"page"},{"location":"","page":"Index","title":"Index","text":"Optimize returns a State datatype which contains some information about the approximation. For instance, you may use mainly two functions to obtain such approximation.","category":"page"},{"location":"","page":"Index","title":"Index","text":"@show minimum(result)\n@show minimizer(result)","category":"page"},{"location":"#Contents","page":"Index","title":"Contents","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Pages = [\"tutorial.md\", \"examples.md\", \"algorithms.md\", \"problems.md\", \"indicators.md\", \"visualization.md\", \"api.md\"]\nDepth = 2","category":"page"},{"location":"#Related-packages","page":"Index","title":"Related packages","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Evolutionary.jl: Genetic algorithms, \"Evolution\" Strategies, among others.\nGeneticAlgorithms.jl: Genetic Algorithms\nBlackBoxOptim.jl: Optimizers for black-box optimization (no information about the objective function).\nNODAL.jl: Stochastic Local Search methods, such as Simulated Annealing and Tabu Search.\nOther Packages.","category":"page"}]
}

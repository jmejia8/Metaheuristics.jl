var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"","category":"page"},{"location":"tutorials/simple-tutorial/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"After reading this tutorial you'll become an expert in using the Metaheuristics module.","category":"page"},{"location":"tutorials/simple-tutorial/#Minimization-Problem","page":"Getting Started","title":"Minimization Problem","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Assume you want to optimize the following minimization problem:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Minimize:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"f(x) = 10D + sum_i=1^D x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"where xin -5 5^D, that is, each coordinate in x is between -5 and 5. Use D=10.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Note that the global optimum is obtained when x_i = 0 for all i. Thus, min f(x) = 0.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Objective function:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"using Metaheuristics # hide\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2pi*x) )","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Bounds:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"bounds = BoxConstrainedSpace(lb = -5ones(10), ub = 5ones(10))","category":"page"},{"location":"tutorials/simple-tutorial/#Providing-Information","page":"Getting Started","title":"Providing Information","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Since the optimum is known, then we can provide this information to the optimizer.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"information = Information(f_optimum = 0.0)","category":"page"},{"location":"tutorials/simple-tutorial/#Common-Settings","page":"Getting Started","title":"Common Settings","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Usually, users could require to limit the number of generations/iterations or the number of function evaluations. To do that, let's assume that the metaheuristic should evaluate at most 9000D times the objective function. Moreover, since information is provided, then we can set the desired accuracy (f(x) - f(x^*) ) to 10^-5.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"options = Options(f_calls_limit = 9000*10, f_tol = 1e-5);\nnothing # hide","category":"page"},{"location":"tutorials/simple-tutorial/#Choose-a-Metaheuristic","page":"Getting Started","title":"Choose a Metaheuristic","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Metaheuristics.jl provides different metaheuristics for optimization such as Evolutionary Centers Algorithm (ECA), Differential Evolution (DE), Particle Swarm Optimization (PSO), etc. In this tutorial, we will use ECA, but you can use another algorithm following the same steps.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"The metaheuristics accept their parameters but share two common and optional settings information and options.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"algorithm = ECA(information = information, options = options)","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"warning: Consider changing the default parameters.\nChange population size for high dimensional problems.","category":"page"},{"location":"tutorials/simple-tutorial/#Optimize","page":"Getting Started","title":"Optimize","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Now, we are able to approximate the optimum. To do that it is necessary to use the optimize function as follows:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"result = optimize(f, bounds, algorithm)","category":"page"},{"location":"tutorials/simple-tutorial/#Get-the-Results","page":"Getting Started","title":"Get the Results","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Once optimize stops, we can get the approximate solutions.","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Approximated minimum:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"fx = minimum(result)","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Approximated minimizer:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"x = minimizer(result)","category":"page"},{"location":"tutorials/simple-tutorial/#Get-Information-about-the-Resulting-Population","page":"Getting Started","title":"Get Information about the Resulting Population","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Sometimes it is useful to analyze the resulting population (for population-based metaheuristics). To do that you can use fvals to get objective function evaluation and positions to get their positions.","category":"page"},{"location":"tutorials/simple-tutorial/#Bonus","page":"Getting Started","title":"Bonus","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"We recommend you wrap your program in a function for performance purposes:","category":"page"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"using Metaheuristics\n\nfunction main()\n    # objective function\n    f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x) )\n    \n    # limits/bounds\n    bounds = BoxConstrainedSpace(lb = -5ones(10), ub = 5ones(10))\n    \n    # information on the minimization problem\n    information = Information(f_optimum = 0.0)\n\n    # generic settings\n    options = Options(f_calls_limit = 9000*10, f_tol = 1e-5)\n    \n    # metaheuristic used to optimize\n    algorithm = ECA(information = information, options = options)\n\n    # start the minimization process\n    result = optimize(f, bounds, algorithm)\n\n    fx = minimum(result)\n    x = minimizer(result)\n\n    x, fx\nend\n\nmain()","category":"page"},{"location":"tutorials/simple-tutorial/#Summary","page":"Getting Started","title":"Summary","text":"","category":"section"},{"location":"tutorials/simple-tutorial/","page":"Getting Started","title":"Getting Started","text":"Now you are able to approximate global optimum solutions using Metaheuristics.","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Please, be free to send me your PR, issue or any comment about this package for Julia.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"You can contribute as follows:","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Implementing new Metaheuristics. See the tutorial \"Create Your Own Metaheuristic\" for creating new metaheuristics.\nImproving the documentation.\nReporting issues here.","category":"page"},{"location":"tutorials/create-metaheuristic/#Create-Your-Own-Metaheuristic","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/#Introduction","page":"Create Your Own Metaheuristic","title":"Introduction","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Firstly, you need to know what occurs when the optimize function is called.","category":"page"},{"location":"tutorials/create-metaheuristic/#Optimization-Process","page":"Create Your Own Metaheuristic","title":"Optimization Process","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Initialization: status = initialize!(status, parameters, problem, information, options) this function should initialize a State with population members according to the parameters provided.\nMain optimization loop: while status.stop == false do\nupdate population, parameters via update_state!(status, parameters, problem, information, options),\nand stop_criteria!(status, parameters, problem, information, options) will change status.stop.\nFinal Stage: When the loop in step 2 breaks, then a final function is called final_stage! for the final update of the state, e.g., delete infeasible solutions in population, get non-dominated solutions, etc. ","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Initialization:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function initialize!(\n                status, # an initialized State (if applicable)\n                parameters::AbstractParameters,\n                problem,\n                information,\n                options,\n                args...;\n                kargs...\n        )\n\n    # initialize the stuff here\n    return State(0.0, zeros(0)) # replace this\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Optimization Process: In this step, the State is updated using the following function which is called at each iteration/generation.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function update_state!(\n        status,\n        parameters::AbstractParameters,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n)\n    # update any element in the State \n    return\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Stopping Criteria: By default, the metaheuristics will stop when either the number of function evaluations or the number of iterations are exceeded. Also, you can establish different criteria via:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function stop_criteria!(status, parameters::MyMetaheuristics, problem, information, options)\n    # your stopping criteria here!\n    #...\n    status.stop = true\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Final Step:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function final_stage!(\n        status,\n        parameters::AbstractParameters,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n)\n    return\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#The-Algorithm-Parameters","page":"Create Your Own Metaheuristic","title":"The Algorithm Parameters","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Any proposed algorithm, let's say \"XYZ\", uses different parameters, then it is suggested to store them in a structure, e.g.:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"# structure with algorithm parameters\nmutable struct XYZ <: AbstractParameters\n    N::Int # population size\n    p_crossover::Float64 # crossover probability\n    p_mutation::Float64 # mutation probability\nend\n\n# a \"constructor\" \nfunction XYZ(;N = 0, p_crossover = 0.9, p_mutation = 0.1)\n    parameters = XYZ(N, p_crossover, p_mutation)\n\n    Algorithm(\n        parameters,\n        information = information,\n        options = options,\n    )\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"If you want to implement an algorithm outside of the Metaheuristics module, then include explicitly the methods you require (or use the Metaheuristics. prefix) as in Step 0, otherwise, go to Step 1.","category":"page"},{"location":"tutorials/create-metaheuristic/#Implementing-a-Simple-Genetic-Algorithm","page":"Create Your Own Metaheuristic","title":"Implementing a Simple Genetic Algorithm","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"The following steps describe how to implement a simple Genetic Algorithm.","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-0","page":"Create Your Own Metaheuristic","title":"Step 0","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Including stuff from Metaheuristics we need.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"# base methods\nusing Metaheuristics\nimport Metaheuristics: initialize!, update_state!, final_stage!\nimport Metaheuristics: AbstractParameters, gen_initial_state, Algorithm, get_position\n# genetic operators\nimport Metaheuristics: SBX_crossover, polynomial_mutation!, create_solution, is_better\nimport Metaheuristics: reset_to_violated_bounds!","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-1:-The-Parameters","page":"Create Your Own Metaheuristic","title":"Step 1: The Parameters","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Since we are creating a simple Genetic Algorithm (GA), let's define the parameters for the GA.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"# structure with algorithm parameters\nmutable struct MyGeneticAlgorithm <: AbstractParameters\n    N::Int # population size\n    p_crossover::Float64 # crossover probability\n    p_mutation::Float64 # mutation probability\nend","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function MyGeneticAlgorithm(;N = 100,\n                            p_crossover = 0.9,\n                            p_mutation = 0.1,\n                            information = Information(),\n                            options = Options()\n    )\n    parameters = MyGeneticAlgorithm(N, p_crossover, p_mutation)\n\n    Algorithm(\n        parameters,\n        information = information,\n        options = options,\n    )\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-2:-Initialization","page":"Create Your Own Metaheuristic","title":"Step 2: Initialization","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Initialize population, parameters and settings before the optimization process begins. The most common initialization method is generating a uniformly distributed random  number within the provided bounds. Here, Metaheuristics.gen_initial_state for that purpose. Note that Metaheuristics.gen_initial_state require that parameters.N is defined.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function initialize!(\n        status,\n        parameters::MyGeneticAlgorithm,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    if options.iterations == 0\n        options.iterations = 500\n    end\n\n    if options.f_calls_limit == 0\n        options.f_calls_limit = options.iterations * parameters.N + 1\n    end\n\n    # gen_initial_state require that `parameters.N` is defined.\n    return gen_initial_state(problem,parameters,information,options,status)\n\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-3:-Evolve-Population","page":"Create Your Own Metaheuristic","title":"Step 3: Evolve Population","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Now, it is time to update (evolve) your population by using genetic operators: selection, crossover, mutation and environmental selection.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function update_state!(\n        status,\n        parameters::MyGeneticAlgorithm,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    population = status.population\n    N = parameters.N\n\n    for i in 1:N\n        # selection\n        parent_1 = get_position(rand(population))\n        parent_2 = get_position(rand(population))\n\n        # generate offspring  via SBX crossover\n        c,_ = SBX_crossover(parent_1, parent_2, problem.bounds, 20, parameters.p_crossover)\n\n        # Mutate solution\n        polynomial_mutation!(c, problem.bounds, 15, parameters.p_mutation)\n        # Fix solution if necessary\n        reset_to_violated_bounds!(c, problem.bounds)\n\n        # crate the solution and evaluate fitness (x, f(x))\n        offspring = create_solution(c, problem)\n\n        push!(population, offspring)\n    end\n\n    # environmental selection\n    sort!(population, lt = is_better, alg=PartialQuickSort(N))\n    deleteat!(population, N+1:length(population))\n\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-4:-After-Evolution","page":"Create Your Own Metaheuristic","title":"Step 4: After Evolution","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"This step is optional, but here is used to get the elite solution aka the best solution found by our GA.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function final_stage!(\n        status,\n        parameters::MyGeneticAlgorithm,\n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    # the first solution is the best one since the population is ordered in the previous step\n    status.best_sol = status.population[1]\n    status.final_time=time()\n    return\nend","category":"page"},{"location":"tutorials/create-metaheuristic/#Step-5:-Time-to-Optimize","page":"Create Your Own Metaheuristic","title":"Step 5: Time to Optimize","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Now, we are able to solve the optimization problem using our genetic algorithm.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"compat: Optimization Problems\nAs you can see, MyGeneticAlgorithm was not restricted to any kind of optimization problems, however works for constrained, unconstrained single- and multi-objective problems; why? The method gen_initial_state creates a State according to the output of the objective function f, whilst is_better is comparing solutions according to the solution type.","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"function main()\n    # test problem\n    f, bounds, _ = Metaheuristics.TestProblems.rastrigin()\n\n    # optimize and get the results\n    res = optimize(f, bounds, MyGeneticAlgorithm())\n    display(res)\nend\n\nmain()","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Output:","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"+=========== RESULT ==========+\n  iteration: 500\n    minimum: 1.41152e-06\n  minimizer: [6.98505513305995e-6, -1.1651666613615994e-5, -4.343967193003195e-6, 3.567365134464557e-5, 1.3393840640183734e-5, 5.591802709915942e-5, -1.477407456986382e-5, 6.325103756718973e-6, 1.9153467328726614e-5, 4.132106648380982e-5]\n    f calls: 50000\n total time: 1.3685 s\n+============================+","category":"page"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"See optimize for more information.","category":"page"},{"location":"tutorials/create-metaheuristic/#Exercises","page":"Create Your Own Metaheuristic","title":"Exercises","text":"","category":"section"},{"location":"tutorials/create-metaheuristic/","page":"Create Your Own Metaheuristic","title":"Create Your Own Metaheuristic","text":"Test your algorithm on a multi-objective optimization problem. Suggestion: change rastrigin by ZDT1.\nImplement an interest metaheuristic and make a PR to the Metaheuristics.jl repo on the Github.","category":"page"},{"location":"indicators/#Performance-Indicators","page":"Performance Indicators","title":"Performance Indicators","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Metaheuristics.jl includes performance indicators to assess evolutionary optimization algorithms performance.","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Available indicators:","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Pages = [ \"indicators.md\"]\nDepth = 3","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Performance Indicators in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"warning: Minimization is always assumed here.\nNote that in Metaheuristics.jl, minimization is always assumed. Therefore these indicators have been developed for minimization problems.","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators","text":"PerformanceIndicators\n\nThis module includes performance indicators to assess evolutionary multi-objective optimization algorithms.\n\ngd Generational Distance.\nigd Inverted Generational Distance.\ngd_plus Generational Distance plus.\nigd_plus Inverted Generational Distance plus.\ncovering Covering indicator (C-metric).\nhypervolume Hypervolume indicator.\n\nExample\n\njulia> import Metaheuristics: PerformanceIndicators, TestProblems\n\njulia> A = [ collect(1:3) collect(1:3) ]\n3×2 Array{Int64,2}:\n 1  1\n 2  2\n 3  3\n\njulia> B = A .- 1\n3×2 Array{Int64,2}:\n 0  0\n 1  1\n 2  2\n\njulia> PerformanceIndicators.gd(A, B)\n0.47140452079103173\n\njulia> f, bounds, front = TestProblems.get_problem(:ZDT1);\n\njulia> front\n                          F space\n         ┌────────────────────────────────────────┐ \n       1 │⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠈⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠈⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠈⠢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠉⠢⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2   │⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠲⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠒⢤⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⠢⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠢⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠢⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠢⢤⣀⠀⠀⠀⠀⠀│ \n       0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⠢⢄⣀│ \n         └────────────────────────────────────────┘ \n         0                                        1\n                            f_1\n\njulia> PerformanceIndicators.igd_plus(front, front)\n0.0\n\n\n\n\n\n","category":"module"},{"location":"indicators/#Generational-Distance","page":"Performance Indicators","title":"Generational Distance","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Generational Distance in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.gd","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.gd","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.gd","text":"gd(front, true_pareto_front; p = 1)\n\nReturns the Generational Distance.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Generational-Distance-Plus","page":"Performance Indicators","title":"Generational Distance Plus","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.gd_plus","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.gd_plus","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.gd_plus","text":"gd_plus(front, true_pareto_front; p = 1)\n\nReturns the Generational Distance Plus.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Inverted-Generational-Distance","page":"Performance Indicators","title":"Inverted Generational Distance","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Inverted Generational Distance in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.igd","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.igd","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.igd","text":"igd(front, true_pareto_front; p = 1)\n\nReturns the Inverted Generational Distance.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Inverted-Generational-Distance-Plus","page":"Performance Indicators","title":"Inverted Generational Distance Plus","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.igd_plus","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.igd_plus","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.igd_plus","text":"igd_plus(front, true_pareto_front; p = 1)\n\nReturns the Inverted Generational Distance Plus.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nState\nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Spacing-Indicator","page":"Performance Indicators","title":"Spacing Indicator","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.spacing","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.spacing","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.spacing","text":"spacing(A)\n\nComputes the Schott spacing indicator. spacing(A) == 0 means that vectors in A are uniformly distributed.\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Covering-Indicator-(C-metric)","page":"Performance Indicators","title":"Covering Indicator (C-metric)","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.covering","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.covering","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.covering","text":"covering(A, B)\n\nComputes the covering indicator (percentage of vectors in B that are dominated by vectors in A) from two sets with non-dominated solutions.\n\nA and B with size (n, m) where n is number of samples and m is the vector dimension.\n\nNote that covering(A, B) == 1 means that all solutions in B are dominated by those in A. Moreover, covering(A, B) != covering(B, A) in general.\n\nIf A::State and B::State, then computes covering(A.population, B.population) after ignoring dominated solutions in each set.\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Hypervolume","page":"Performance Indicators","title":"Hypervolume","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"(Image: Hypervolume Indicator in Julia)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.hypervolume","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.hypervolume","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.hypervolume","text":"hypervolume(front, reference_point)\n\nComputes the hypervolume indicator, i.e., volume between points in front and reference_point.\n\nNote that each point in front must (weakly) dominates to reference_point. Also, front is a non-dominated set.\n\nIf front::State and reference_point::Vector, then computes hypervolume(front.population, reference_point) after ignoring solutions in front that do not dominate reference_point.\n\n\n\n\n\n","category":"function"},{"location":"indicators/#Examples","page":"Performance Indicators","title":"Examples","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Computing hypervolume indicator from vectors in a Matrix","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"import Metaheuristics.PerformanceIndicators: hypervolume\n\nf1 = collect(0:10); # objective 1\nf2 = 10 .- collect(0:10); # objective 2\n\nfront = [ f1 f2 ] \n\nreference_point = [11, 11]\n\nhv = hypervolume(front, reference_point)","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Now, let's compute the hypervolume implementation in Julia from the result of  NSGA3 when solving DTLZ2 test problem.","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"using Metaheuristics\nimport Metaheuristics.PerformanceIndicators: hypervolume\nimport Metaheuristics: TestProblems, get_non_dominated_solutions\n\nf, bounds, true_front = TestProblems.DTLZ2();\n\nresult = optimize(f, bounds, NSGA3());\n\napprox_front = get_non_dominated_solutions(result.population)\n\nreference_point = nadir(result.population)\n\nhv = hypervolume(approx_front, reference_point)","category":"page"},{"location":"indicators/#\\Delta_p-(Delta-p)","page":"Performance Indicators","title":"Delta_p (Delta p)","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":" Metaheuristics.PerformanceIndicators.deltap","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.deltap","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.deltap","text":"deltap(front, true_pareto_front; p = 1)\nΔₚ(front, true_pareto_front; p = 1)\n\nReturns the averaged Hausdorff distance indicator aka Δₚ (Delta p).\n\n\"Δₚ\" can be typed as \\Delta<tab>\\_p<tab>.\n\nParameters\n\nfront and true_pareto_front can be:\n\nN×m matrix where N is the number of points and m is the number of objectives. \nArray{xFgh_indiv} (usually State.population)\n\n\n\n\n\n","category":"function"},{"location":"indicators/#\\varepsilon-Indicator","page":"Performance Indicators","title":"varepsilon-Indicator","text":"","category":"section"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Unary and binary varepsilon-indicator (epsilon-indicator). Details in [17]","category":"page"},{"location":"indicators/","page":"Performance Indicators","title":"Performance Indicators","text":"Metaheuristics.PerformanceIndicators.epsilon_indicator","category":"page"},{"location":"indicators/#Metaheuristics.PerformanceIndicators.epsilon_indicator","page":"Performance Indicators","title":"Metaheuristics.PerformanceIndicators.epsilon_indicator","text":"epsilon_indicator(A, B)\n\nComputes the ε-indicator for non-dominated sets A and B. It is assumed that all values in A and B are positive. If negative, the sets are translated to positive values.\n\nInterpretation\n\nepsilon_indicator(A, PF) is unary if PF is the Pareto-optimal front.\nepsilon_indicator(A, B) == 1 none is better than the other.\nepsilon_indicator(A, B) < 1 means that A is better than B.\nepsilon_indicator(A, B) > 1 means that B is better than A.\nValues closer to 1 are preferable.\n\nExamples\n\njulia> A1 = [4 7;5 6;7 5; 8 4.0; 9 2];\n\njulia> A2 = [4 7;5 6;7 5; 8 4.0];\n\njulia> A3 = [6 8; 7 7;8 6; 9 5;10 4.0 ];\n\njulia> PerformanceIndicators.epsilon_indicator(A1, A2)\n1.0\n\njulia> PerformanceIndicators.epsilon_indicator(A1, A3)\n0.9\n\njulia> f, bounds, pf = Metaheuristics.TestProblems.ZDT3();\n\njulia> res = optimize(f, bounds, NSGA2());\n\njulia> PerformanceIndicators.epsilon_indicator(res, pf)\n1.00497701620997\n\n\n\n\n\n","category":"function"},{"location":"visualization/#Visualization","page":"Visualization","title":"Visualization","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Solving ZDT6 using SMS-EMOA in Julia)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Presenting the results using fancy plots is an important part of solving optimization problems. In this part, we use the Plots.jl package which can be installed via de Pkg prompt within Julia:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Type ] and then:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"pkg> add Plots","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Or:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"julia> import Pkg; Pkg.add(\"Plots\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Once Plots is installed on your Julia distribution, you will be able to reproduce the  following examples.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Assume you want to solve the following minimization problem.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Rastrigin Surface)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Minimize:","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"f(x) = 10D + sum_i=1^D  x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"where xin-5 5^D, i.e., -5 leq x_i leq 5 for i=1ldotsD. D is the dimension number, assume D=10.","category":"page"},{"location":"visualization/#Population-Distribution","page":"Visualization","title":"Population Distribution","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Let's solve the above optimization problem and plot the resulting population (projecting two specific dimensions).","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\n# positions in matrix NxD \nX = positions(result)\n\nscatter(X[:,1], X[:,2], label=\"Population\")\n\nx = minimizer(result)\nscatter!(x[1:1], x[2:2], label=\"Best solution\")\n\n\n# (optional) save figure\nsavefig(\"final-population.png\")\n","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"If your optimization problem is scalable, then you also can plot level curves. In this case, let's assume that D=2.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 2\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\n# positions in matrix NxD \nX = positions(result)\n\nxy = range(-5, 5, length=100)\ncontour(xy, xy, (a,b) -> f([a, b]))\n\nscatter!(X[:,1], X[:,2], label=\"Population\")\n\nx = minimizer(result)\nscatter!(x[1:1], x[2:2], label=\"Best solution\")\n\n\n# (optional) save figure\nsavefig(\"final-population-contour.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Objective-Function-Values","page":"Visualization","title":"Objective Function Values","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Metaheuristics.jl implements some methods to obtain the objective function values (fitness) from the solutions in the resulting population. One of the most useful methods is fvals. In this case, let's use PSO.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1)\n\n# Optimizing\nresult = optimize(f, bounds, PSO(options=options))\n\nf_values = fvals(result)\nplot(f_values)\n\n# (optional) save figure\nsavefig(\"fvals.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Convergence","page":"Visualization","title":"Convergence","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Sometimes, it is useful to plot the convergence plot at the end of the optimization process. To do that, it is necessary to set store_convergence = true in Options. Metaheuristics implements a method called convergence.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1, store_convergence = true)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\nf_calls, best_f_value = convergence(result)\n\nplot(xlabel=\"f calls\", ylabel=\"fitness\", title=\"Convergence\")\nplot!(f_calls, best_f_value, label=\"ECA\")\n\n# (optional) save figure\nsavefig(\"convergence.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Convergence)","category":"page"},{"location":"visualization/#Animate-convergence","page":"Visualization","title":"Animate convergence","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"Also, you can plot the population and convergence in the same figure.","category":"page"},{"location":"visualization/#Single-Objective-Problem","page":"Visualization","title":"Single-Objective Problem","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"using Metaheuristics\nusing Plots\ngr()\n\n\n# objective function\nf(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\n\n# number of variables (dimension)\nD = 10\n\n# bounds\nbounds = [-5ones(D) 5ones(D)]'\n\n# Common options\noptions = Options(seed=1, store_convergence = true)\n\n# Optimizing\nresult = optimize(f, bounds, ECA(options=options))\n\nf_calls, best_f_value = convergence(result)\n\nanimation = @animate for i in 1:length(result.convergence)\n    l = @layout [a b]\n    p = plot( layout=l)\n\n    X = positions(result.convergence[i])\n    scatter!(p[1], X[:,1], X[:,2], label=\"\", xlim=(-5, 5), ylim=(-5,5), title=\"Population\")\n    x = minimizer(result.convergence[i])\n    scatter!(p[1], x[1:1], x[2:2], label=\"\")\n\n    # convergence\n    plot!(p[2], xlabel=\"Generation\", ylabel=\"fitness\", title=\"Gen: $i\")\n    plot!(p[2], 1:length(best_f_value), best_f_value, label=false)\n    plot!(p[2], 1:i, best_f_value[1:i], lw=3, label=false)\n    x = minimizer(result.convergence[i])\n    scatter!(p[2], [i], [minimum(result.convergence[i])], label=false)\nend\n\n# save in different formats\n# gif(animation, \"anim-convergence.gif\", fps=30)\nmp4(animation, \"anim-convergence.mp4\", fps=30)\n","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: )","category":"page"},{"location":"visualization/#Multi-Objective-Problem","page":"Visualization","title":"Multi-Objective Problem","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, SMS_EMOA, TestProblems, pareto_front, Options\nimport Metaheuristics.PerformanceIndicators: Δₚ\nusing Plots; gr()\n\n# get test function\nf, bounds, pf = TestProblems.ZDT6();\n\n# optimize using SMS-EMOA\nresult = optimize(f, bounds, SMS_EMOA(N=70,options=Options(iterations=500,seed=0, store_convergence=true)))\n\n# true pareto front\nB = pareto_front(pf)\n# error to the true front\nerr = [ Δₚ(r.population, pf) for r in result.convergence]\n# generate plots\na = @animate for i in 1:5:length(result.convergence)\n    A = pareto_front(result.convergence[i])\n\n    p = plot(B[:, 1], B[:,2], label=\"True Pareto Front\", lw=2,layout=(1,2), size=(850, 400))\n    scatter!(p[1], A[:, 1], A[:,2], label=\"SMS-EMOA\", markersize=4, color=:black, title=\"ZDT6\")\n    plot!(p[2], eachindex(err), err, ylabel=\"Δₚ\", legend=false)\n    plot!(p[2], 1:i, err[1:i], title=\"Generation $i\")\n    scatter!(p[2], [i], err[i:i])\nend\n\n# save animation\ngif(a, \"ZDT6.gif\", fps=20)","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Soving ZDT6 using SMS-EMOA in Julia)","category":"page"},{"location":"visualization/#Pareto-Front","page":"Visualization","title":"Pareto Front","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, NSGA2, TestProblems, pareto_front, Options\nusing Plots; gr()\n\nf, bounds, solutions = TestProblems.ZDT3();\n\nresult = optimize(f, bounds, NSGA2(options=Options(seed=0)))\n\nA = pareto_front(result)\nB = pareto_front(solutions)\n\nscatter(A[:, 1], A[:,2], label=\"NSGA-II\")\nplot!(B[:, 1], B[:,2], label=\"Parento Front\", lw=2)\nsavefig(\"pareto.png\")","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"(Image: Final Population)","category":"page"},{"location":"visualization/#Live-Plotting","page":"Visualization","title":"Live Plotting","text":"","category":"section"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"The optimize function has a keyword parameter named logger that contains a function pointer. Such function will receive the State at the end of each iteration in the main optimization loop.","category":"page"},{"location":"visualization/","page":"Visualization","title":"Visualization","text":"import Metaheuristics: optimize, NSGA2, TestProblems, pareto_front, Options, fvals\nusing Plots; gr()\n\nf, bounds, solutions = TestProblems.ZDT3();\npf = pareto_front(solutions)\n\nlogger(st) = begin\n    A = fvals(st)\n    scatter(A[:, 1], A[:,2], label=\"NSGA-II\", title=\"Gen: $(st.iteration)\")\n    plot!(pf[:, 1], pf[:,2], label=\"Parento Front\", lw=2)\n    gui()\n    sleep(0.1)\nend\n\nresult = optimize(f, bounds, NSGA2(options=Options(seed=0)), logger=logger)\n","category":"page"},{"location":"tutorials/parallelization/#Parallelization","page":"Parallelization","title":"Parallelization","text":"","category":"section"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Evaluating multiple solutions at the same time can reduce computational time. This tutorial shows how to implement parallel evaluations.","category":"page"},{"location":"tutorials/parallelization/#Threads:-Single-Objective-Optimization","page":"Parallelization","title":"Threads: Single-Objective Optimization","text":"","category":"section"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"First, open Julia with the command julia -t4 if you have 4 threads available (select the number of threads regarding your computer capabilities).","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Let's assume that you have an expensive objective function that takes a decision vector.","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> f(x) = begin\n           sleep(1)\n           sum(x)\n       end","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Without parallelization:","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> optimize(f, [-10ones(5) 10ones(5)], ECA(options=Options(iterations=10)))\n\n+=========== RESULT ==========+\n  iteration: 10\n    minimum: -41.6528\n  minimizer: [-9.397440235044382, -9.928665036618323, -4.635292227471077, -8.901490335239036, -8.789883453491104]\n    f calls: 350\n total time: 350.7866 s\nstop reason: Maximum number of iterations exceeded.\n+============================+","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"To evaluate f in different threads, you only need to perform a batch evaluation as follows. Note that set parallel_evaluation = true in Options() is mandatory for batch evaluations.","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> function f_parallel(X)\n           fitness = zeros(size(X,1))\n           Threads.@threads for i in 1:size(X,1)\n               fitness[i] = f(X[i,:])\n           end\n           fitness\n       end\n\njulia> options = Options(iterations=10, parallel_evaluation=true);\n\njulia> optimize(f_parallel, [-10ones(5) 10ones(5)], ECA(;options))\n+=========== RESULT ==========+\n  iteration: 10\n    minimum: -40.3347\n  minimizer: [-8.584413535451635, -7.376644826521645, -7.756398645618526, -9.066386783877238, -7.550888765058555]\n    f calls: 350\n total time: 90.2019 s\nstop reason: Maximum number of iterations exceeded.\n+============================+","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"You can see 300% (approx) of speed up using 4 threads.","category":"page"},{"location":"tutorials/parallelization/#Threads:-Constrained-Optimization","page":"Parallelization","title":"Threads: Constrained Optimization","text":"","category":"section"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"This part gives how to evaluate constraints in different threads.","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> function f_parallel(X)\n           N = size(X,1)\n           fx, gx, hx = zeros(N), zeros(N,2), zeros(N,1)\n           Threads.@threads for i in 1:size(X,1)\n               x = X[i,:]\n               fx[i] = sum(x.^2)     # objective function\n               gx[i, 1] = sum(x) - 1 # constraint 1\n               gx[i, 2] = 1 - sum(x) # constraint 2\n           end\n           fx, gx, hx\n       end\n\njulia> options = Options(parallel_evaluation=true);\n\njulia> optimize(f_parallel, [-10ones(5) 10ones(5)], ECA(;options))\n+=========== RESULT ==========+\n  iteration: 1345\n    minimum: 0.2\n  minimizer: [0.20000000134830526, 0.19999999986544995, 0.1999999991116526, 0.19999999921887918, 0.20000000045571298]\n    f calls: 47075\n  feasibles: 35 / 35 in final population\n total time: 0.8151 s\nstop reason: Small difference of objective function values.\n+============================+","category":"page"},{"location":"tutorials/parallelization/#Threads:-Multi-objective-Optimization","page":"Parallelization","title":"Threads: Multi-objective Optimization","text":"","category":"section"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Assume that f is an expensive function.","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> f, bounds, pf = Metaheuristics.TestProblems.ZDT3();","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Let's implement a function that evaluates f in multiple threads.","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> function f_parallel(X)\n                  N = size(X,1)\n                  nobjectives = 2\n                  fx, gx, hx = zeros(N,nobjectives), zeros(N,1), zeros(N,1)\n                  Threads.@threads for i in 1:N\n                      fx[i,:], gx[i,:], hx[i,:] = f(X[i,:])\n                  end\n                  fx, gx, hx\n              end\nf_parallel (generic function with 1 method)","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Now, optimize f_parallel.","category":"page"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"julia> options = Options(parallel_evaluation=true); # this is important\n\njulia> optimize(f_parallel, bounds, NSGA2(;options))\n+=========== RESULT ==========+\n  iteration: 500\n population:         ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀F space⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n         ┌────────────────────────────────────────┐ \n       2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⢣⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠈⠢⠤⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f₂    │⠀⠀⠀⠀⠀⠀⠀⠀⠘⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠣⡀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠻⠍⠉⠉⠉⠉⠉⠉⢭⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢇⠀⠀│ \n      -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         └────────────────────────────────────────┘ \n         ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0.9⠀ \n         ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀f₁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \nnon-dominated solution(s):\n         ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀F space⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n         ┌────────────────────────────────────────┐ \n       2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⢣⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠈⠢⠤⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f₂    │⠀⠀⠀⠀⠀⠀⠀⠀⠘⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠣⡀⠀⠀⠀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠻⠍⠉⠉⠉⠉⠉⠉⢭⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠀⠀⠀⠀⠀⠀⠀⢰⠀⠀⠀│ \n         │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢇⠀⠀│ \n      -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n         └────────────────────────────────────────┘ \n         ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀0.9⠀ \n         ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀f₁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n    f calls: 50000\n  feasibles: 100 / 100 in final population\n total time: 1.8363 s\nstop reason: Maximum objective function calls exceeded.\n+============================+","category":"page"},{"location":"tutorials/parallelization/#Distributed","page":"Parallelization","title":"Distributed","text":"","category":"section"},{"location":"tutorials/parallelization/","page":"Parallelization","title":"Parallelization","text":"Contributions are welcome. See \"Contributing\" for more details.","category":"page"},{"location":"tutorials/n-queens/#N-Queens","page":"N-Queens","title":"N-Queens","text":"","category":"section"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"In the N-queen problem, N queens must be placed on an Ntimes N chessboard without interfering with each other. There can never be more than one queen on a chess row, column or diagonal because of the queen's ability to move vertically, horizontally and diagonally. ","category":"page"},{"location":"tutorials/n-queens/#Solution-Representation","page":"N-Queens","title":"Solution Representation","text":"","category":"section"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"A naive representation is to use a binary matrix to represent a chessboard c_ij where c_ij=1 would indicate a queen in row i and column j; however the search space cardinality (number of solutions) is huge for this representation. A permutation-based representation can be used instead. A permutation will contain in position j the row i where a queen is, removing a lot of infeasible solutions (queens attacking horizontally and vertically).","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"Let's compute the number of solutions (a.k.a. cardinality):","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"using Metaheuristics\nN = 8 # for 8-queens\ncardinality(BitArraySpace(N*N))\ncardinality(PermutationSpace(N))","category":"page"},{"location":"tutorials/n-queens/#Objective-Function","page":"N-Queens","title":"Objective Function","text":"","category":"section"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"It is necessary to minimize the number of attacks, the following objective function is proposed to this end:","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"function attacks(chessboard)\n    N = length(chessboard)\n    n_attacks = 0\n    # check attack in both diagonas for each queen\n    for i = 1:N\n        Δrows = i + chessboard[i]\n        Δcols = i - chessboard[i]\n        for j = (i+1):N\n            # check diagonal [\\]\n            n_attacks +=  j + chessboard[j] == Δrows ? 1 : 0\n            # check inverse diagonal [/]\n            n_attacks +=  j - chessboard[j] == Δcols ? 1 : 0\n        end\n    end\n    2n_attacks\nend","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"It can observed that horizontal and vertical are not considered due to the adopted solution representation.","category":"page"},{"location":"tutorials/n-queens/#Let's-find-a-solution","page":"N-Queens","title":"Let's find a solution","text":"","category":"section"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"To minimize the number of attacks, let's use a Genetic Algorithm:","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"using Metaheuristics # hide\nN = 8\noptimize(attacks, PermutationSpace(N), GA); # hide\nresult = optimize(attacks, PermutationSpace(N), GA)","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"It can be observed that the number of attacks is:","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"n_attacks = minimum(result)","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"The optimal permutation is:","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"perm = minimizer(result)","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"Corresponding chessboard configuration:","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"chessboard = zeros(Bool, N, N)\nfor (i,j) = enumerate(perm); chessboard[i,j] = true;end\nchessboard","category":"page"},{"location":"tutorials/n-queens/#The-20-queens-case","page":"N-Queens","title":"The 20-queens case","text":"","category":"section"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"N = 20\nperm = optimize(attacks, PermutationSpace(N), GA, seed=1) |> minimizer","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"attacks(perm)","category":"page"},{"location":"tutorials/n-queens/","page":"N-Queens","title":"N-Queens","text":"chessboard = zeros(Bool, N, N)\nfor (i,j) = enumerate(perm); chessboard[i,j] = true;end\nchessboard","category":"page"},{"location":"faq/#FAQ","page":"FAQ","title":"FAQ","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"Answers to Frequently Asked Questions:","category":"page"},{"location":"faq/#How-to-solve-combinatorial-problems?","page":"FAQ","title":"How to solve combinatorial problems?","text":"","category":"section"},{"location":"faq/","page":"FAQ","title":"FAQ","text":"This package was initially developed for numerical optimization, but recent updates can handle combinatorial problems. The Genetic Algorithm framework (GA) can be used.","category":"page"},{"location":"api/#API-References","page":"API References","title":"API References","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"optimize","category":"page"},{"location":"api/#Metaheuristics.optimize","page":"API References","title":"Metaheuristics.optimize","text":"  optimize(\n        f::Function, # objective function\n        search_space,\n        method::AbstractAlgorithm = ECA();\n        logger::Function = (status) -> nothing,\n  )\n\nMinimize a n-dimensional function f with domain search_space (2×n matrix) using method = ECA() by default.\n\nExample\n\nMinimize f(x) = Σx² where x ∈ [-10, 10]³.\n\nSolution:\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> result = optimize(f, bounds)\n+=========== RESULT ==========+\n  iteration: 1429\n    minimum: 2.5354499999999998e-222\n  minimizer: [-1.5135301653303966e-111, 3.8688354844737692e-112, 3.082095708730726e-112]\n    f calls: 29989\n total time: 0.1543 s\n+============================+\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"optimize!","category":"page"},{"location":"api/#Metaheuristics.optimize!","page":"API References","title":"Metaheuristics.optimize!","text":"optimize!(f, search_space, method;logger)\n\nPerform an iteration of method, and save the results in method.status.\n\nExample\n\nf, bounds, _ = Metaheuristics.TestProblems.sphere();\nmethod = ECA()\nwhile !Metaheuristics.should_stop(method)\n    optimize!(f, bounds, method)\nend\nresult = Metaheuristics.get_result(method)\n\nSee also optimize.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"State","category":"page"},{"location":"api/#Metaheuristics.State","page":"API References","title":"Metaheuristics.State","text":"State datatype\n\nState is used to store the current metaheuristic status. In fact, the optimize function returns a State.\n\nbest_sol Stores the best solution found so far.\npopulation is an Array{typeof(best_sol)} for population-based algorithms.\nf_calls is the number of objective functions evaluations.\ng_calls  is the number of inequality constraints evaluations.\nh_calls is the number of equality constraints evaluations.\niteration is the current iteration.\nsuccess_rate percentage of new generated solutions better that their parents. \nconvergence used save the State at each iteration.\nstart_time saves the time() before the optimization process.\nfinal_time saves the time() after the optimization process.\nstop if true, then stops the optimization process.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds)\n+=========== RESULT ==========+\n| Iter.: 1009\n| f(x) = 7.16271e-163\n| solution.x = [-7.691251412064516e-83, 1.0826961235605951e-82, -8.358428300092186e-82]\n| f calls: 21190\n| Total time: 0.2526 s\n+============================+\n\njulia> minimum(state)\n7.162710802659093e-163\n\njulia> minimizer(state)\n3-element Array{Float64,1}:\n -7.691251412064516e-83\n  1.0826961235605951e-82\n -8.358428300092186e-82\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Information","category":"page"},{"location":"api/#Metaheuristics.Information","page":"API References","title":"Metaheuristics.Information","text":"Information Structure\n\nInformation can be used to store the true optimum in order to stop a metaheuristic early.\n\nProperties:\n\nf_optimum known minimum.\nx_optimum known minimizer.\n\nIf Options is provided, then optimize will stop when |f(x) - f(x_optimum)| < Options.f_tol or ‖ x - x_optimum ‖ < Options.x_tol (euclidean distance).\n\nExample\n\nIf you want an approximation to the minimum with accuracy of 1e-3 (|f(x) - f(x*)| < 1e-3), then you may use Information.\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> information = Information(f_optimum = 0.0)\nInformation(0.0, Float64[])\n\njulia> options = Options(f_tol = 1e-3)\nOptions(0.0, 0.001, 0.0, 0.0, 1000.0, 0.0, 0.0, 0, false, true, false, :minimize)\n\njulia> state = optimize(f, bounds, ECA(information=information, options=options))\n+=========== RESULT ==========+\n| Iter.: 22\n| f(x) = 0.000650243\n| solution.x = [0.022811671589729583, 0.007052331140376011, -0.008951836265056107]\n| f calls: 474\n| Total time: 0.0106 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Options","category":"page"},{"location":"api/#Metaheuristics.Options","page":"API References","title":"Metaheuristics.Options","text":"Options(;\n    x_tol::Real = 1e-8,\n    f_tol::Real = 1e-12,\n    f_tol_rel::Real = eps(),\n    f_tol_abs::Real = 0.0,\n    g_tol::Real = 0.0,\n    h_tol::Real = 0.0,\n    f_calls_limit::Real = 0,\n    time_limit::Real = Inf,\n    iterations::Int = 1,\n    store_convergence::Bool = false,\n    debug::Bool = false,\n    seed = rand(UInt),\n    rng  = default_rng_mh(seed),\n    parallel_evaluation = false,\n    verbose = false,\n)\n\nOptions stores common settings for metaheuristics such as the maximum number of iterations debug options, maximum number of function evaluations, etc.\n\nMain properties:\n\nx_tol tolerance to the true minimizer if specified in Information.\nf_tol tolerance to the true minimum if specified in Information.\nf_tol_rel relative tolerance.\nf_calls_limit is the maximum number of function evaluations limit.\ntime_limit is the maximum time that optimize can spend in seconds.\niterations is the maximum number of allowed iterations.\nstore_convergence if true, then push the current State in State.convergence at each generation/iteration\ndebug if true, then optimize function reports the current State (and interest information) for each iterations.\nseed non-negative integer for the random generator seed.\nparallel_evaluation enables batch evaluations.\nverbose show simplified results each iteration.\nrng user-defined Random Number Generator.\n\nExample\n\njulia> options = Options(f_calls_limit = 1000, debug=false, seed=1);\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds, ECA(options=options));\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"convergence","category":"page"},{"location":"api/#Metaheuristics.convergence","page":"API References","title":"Metaheuristics.convergence","text":"convergence(state)\n\nget the data (touple with the number of function evaluations and fuction values) to plot the convergence graph. \n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> bounds = [  -10.0 -10 -10; # lower bounds\n                    10.0  10 10 ] # upper bounds\n2×3 Array{Float64,2}:\n -10.0  -10.0  -10.0\n  10.0   10.0   10.0\n\njulia> state = optimize(f, bounds, ECA(options=Options(store_convergence=true)))\n+=========== RESULT ==========+\n| Iter.: 1022\n| f(x) = 7.95324e-163\n| solution.x = [-7.782044850211721e-82, 3.590044165897827e-82, -2.4665318114710003e-82]\n| f calls: 21469\n| Total time: 0.3300 s\n+============================+\n\njulia> n_fes, fxs = convergence(state);\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"minimizer","category":"page"},{"location":"api/#Metaheuristics.minimizer","page":"API References","title":"Metaheuristics.minimizer","text":"minimizer(state)\n\nReturns the approximation to the minimizer (argmin f(x)) stored in state.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"minimum(state::State)","category":"page"},{"location":"api/#Base.minimum-Tuple{State}","page":"API References","title":"Base.minimum","text":"minimum(state::Metaheuristics.State)\n\nReturns the approximation to the minimum (min f(x)) stored in state.\n\n\n\n\n\n","category":"method"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.TerminationStatusCode","category":"page"},{"location":"api/#Metaheuristics.TerminationStatusCode","page":"API References","title":"Metaheuristics.TerminationStatusCode","text":"TerminationStatusCode\n\nAn Enum of possible => values for State. Possible values:\n\nITERATION_LIMIT\nTIME_LIMIT\nEVALUATIONS_LIMIT\nACCURACY_LIMIT\nOBJECTIVE_VARIANCE_LIMIT\nOBJECTIVE_DIFFERENCE_LIMIT\nOTHER_LIMIT\nUNKNOWN_STOP_REASON\n\nSee also termination_status_message.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"termination_status_message","category":"page"},{"location":"api/#Metaheuristics.termination_status_message","page":"API References","title":"Metaheuristics.termination_status_message","text":"termination_status_message(status)\n\nReturn a string of the message related to the status.\n\nSee TerminationStatusCode\n\nExample:\n\njulia> termination_status_message(Metaheuristics.ITERATION_LIMIT)\n\"Maximum number of iterations exceeded.\"\n\njulia> termination_status_message(optimize(f, bounds))\n\"Maximum number of iterations exceeded.\"\n\njulia> termination_status_message(ECA())\n\"Unknown stop reason.\"\n\n\n\n\n\n","category":"function"},{"location":"api/#Methods-for-Solutions/Individuals","page":"API References","title":"Methods for Solutions/Individuals","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"positions","category":"page"},{"location":"api/#Metaheuristics.positions","page":"API References","title":"Metaheuristics.positions","text":"positions(state)\n\nIf state.population has N solutions, then returns a N×d Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"fvals","category":"page"},{"location":"api/#Metaheuristics.fvals","page":"API References","title":"Metaheuristics.fvals","text":"fvals(state)\n\nIf state.population has N solutions, then returns a Vector with the  objective function values from items in state.population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"nfes","category":"page"},{"location":"api/#Metaheuristics.nfes","page":"API References","title":"Metaheuristics.nfes","text":"nfes(state)\n\nget the number of function evaluations.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.create_child","category":"page"},{"location":"api/#Metaheuristics.create_child","page":"API References","title":"Metaheuristics.create_child","text":"Metaheuristics.create_child(x, fx)\n\nConstructor for a solution depending on the result of fx.\n\nExample\n\njulia> import Metaheuristics\n\njulia> Metaheuristics.create_child(rand(3), 1.0)\n| f(x) = 1\n| solution.x = [0.2700437125780806, 0.5233263210622989, 0.12871108215859772]\n\njulia> Metaheuristics.create_child(rand(3), (1.0, [2.0, 0.2], [3.0, 0.3]))\n| f(x) = 1\n| g(x) = [2.0, 0.2]\n| h(x) = [3.0, 0.3]\n| x = [0.9881102595664819, 0.4816273348099591, 0.7742585077942159]\n\njulia> Metaheuristics.create_child(rand(3), ([-1, -2.0], [2.0, 0.2], [3.0, 0.3]))\n| f(x) = [-1.0, -2.0]\n| g(x) = [2.0, 0.2]\n| h(x) = [3.0, 0.3]\n| x = [0.23983577719146854, 0.3611544510766811, 0.7998754930109109]\n\njulia> population = [ Metaheuristics.create_child(rand(2), (randn(2),  randn(2), rand(2))) for i = 1:100  ]\n                           F space\n          ┌────────────────────────────────────────┐ \n        2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⠂⠀⠀⠀⠀⠀⡇⠈⡀⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠘⠀⡇⠀⠀⠘⠀⠄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠂⠀⠂⠀⠀⢀⠠⠐⠀⡇⠄⠁⠀⠀⠀⡀⠀⢁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠂⢈⠀⠈⡇⠀⡐⠃⠀⠄⠄⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⠄⢐⠠⠀⡄⠀⠀⡇⠀⠂⠈⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠉⠉⠉⠉⠋⠉⠉⠉⠉⠉⠉⠙⢉⠉⠙⠉⠉⡏⠉⠉⠩⠋⠉⠩⠉⠉⠉⡉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉⠉│ \n   f_2    │⠀⠀⠀⠀⠀⡀⠀⠀⠀⠄⠀⠀⡀⠀⠀⠂⠀⡇⠀⠀⠀⠐⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠄⠀⠀⠐⡇⠠⠀⠀⠀⠈⢀⠄⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠂⠀⠄⠀⡀⠀⠂⡇⠐⠘⠈⠂⠀⠈⡀⠀⠀⠀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠄⠀⠀⠀⠀⠀⠂⠀⠂⠀⠀⡇⠀⠈⢀⠐⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠀⠀⠀⠀⠀⢁⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠠⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n       -3 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          -3                                       4\n                             f_1\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"get_position","category":"page"},{"location":"api/#Metaheuristics.get_position","page":"API References","title":"Metaheuristics.get_position","text":"get_position(solution)\n\nGet the position vector.\n\n\n\n\n\nget_position(bee)\n\nGet the position vector of a bee when optimize using ABC algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"fval","category":"page"},{"location":"api/#Metaheuristics.fval","page":"API References","title":"Metaheuristics.fval","text":"fval(solution)\n\nGet the objective function value (fitness) of a solution.\n\n\n\n\n\nfval(solution)\n\nGet the fitness of a bee when optimize using ABC algorithm.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.gval","category":"page"},{"location":"api/#Metaheuristics.gval","page":"API References","title":"Metaheuristics.gval","text":"gval(solution)\n\nGet the inequality constraints of a solution.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.hval","category":"page"},{"location":"api/#Metaheuristics.hval","page":"API References","title":"Metaheuristics.hval","text":"hval(solution)\n\nGet the equality constraints of a solution.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.is_feasible","category":"page"},{"location":"api/#Metaheuristics.is_feasible","page":"API References","title":"Metaheuristics.is_feasible","text":"is_feasible(solution)\n\nReturns true if solution is feasible, otherwise returns false.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.is_better","category":"page"},{"location":"api/#Metaheuristics.is_better","page":"API References","title":"Metaheuristics.is_better","text":"is_better(A, B)\nreturn true if A is better than B in a minimization problem.\nFeasibility rules and dominated criteria are used in comparison.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.dominates","category":"page"},{"location":"api/#Metaheuristics.dominates","page":"API References","title":"Metaheuristics.dominates","text":"does A dominate B?\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.compare","category":"page"},{"location":"api/#Metaheuristics.compare","page":"API References","title":"Metaheuristics.compare","text":"compare(a, b)\ncompares whether two vectors are dominated or not.\nOutput:\n`1` if argument 1 (a) dominates argument 2 (b).\n`2` if argument 2 (b) dominates argument 1 (a).\n`3` if both arguments 1 (a) and 2 (b) are incomparable.\n`0` if both arguments 1 (a) and 2 (b) are equal.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.gen_initial_state","category":"page"},{"location":"api/#Metaheuristics.gen_initial_state","page":"API References","title":"Metaheuristics.gen_initial_state","text":"gen_initial_state(problem,parameters,information,options)\n\nGenerate an initial state, i.e., compute uniformly distributed random vectors in bounds, after that are evaluated in objective function. This method require that parameters.N is valid attribute.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"set_user_solutions!","category":"page"},{"location":"api/#Metaheuristics.set_user_solutions!","page":"API References","title":"Metaheuristics.set_user_solutions!","text":"set_user_solutions!(optimizer, x, fx;verbose=true)\n\nProvide initial solutions to the optimizer.\n\nx can be a Vector and fx a function or fx = f(x)\nx can be a matrix containing solutions in rows.\n\nExample\n\njulia> f(x) = abs(x[1]) + x[2]  + x[3]^2 # objective function\nf (generic function with 1 method)\n\njulia> algo  = ECA(N = 61); # optimizer\n\njulia> # one solution can be provided\n       x0 = [0.5, 0.5, 0.5];\n\njulia> set_user_solutions!(algo, x0, f);\n\njulia> # providing multiple solutions\n       X0 = rand(30, 3); # 30 solutions with dim 3\n\njulia> set_user_solutions!(algo, X0, f);\n\njulia> optimize(f, [0 0 0; 1 1 1.0], algo)\n+=========== RESULT ==========+\n  iteration: 413\n    minimum: 0\n  minimizer: [0.0, 0.0, 0.0]\n    f calls: 25132\n total time: 0.0856 s\nstop reason: Small difference of objective function values.\n+============================+\n\n\n\n\n\n","category":"function"},{"location":"api/#Variation","page":"API References","title":"Variation","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.SBX","category":"page"},{"location":"api/#Metaheuristics.SBX","page":"API References","title":"Metaheuristics.SBX","text":"SBX(;η, p, bounds)\n\nSimulated Binomial Crossover.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.SBX_crossover","category":"page"},{"location":"api/#Metaheuristics.SBX_crossover","page":"API References","title":"Metaheuristics.SBX_crossover","text":"SBX_crossover(vector1, vector2, bounds, η=15, p_variable = 0.9)\n\nSimulated binomial crossover for given two Vectors{Real}.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.ECA_operator","category":"page"},{"location":"api/#Metaheuristics.ECA_operator","page":"API References","title":"Metaheuristics.ECA_operator","text":"ECA_operator(population, K, η_max)\n\nCompute a solution using ECA variation operator, K is the number of solutions used to calculate the center of mass and η_max is the maximum stepsize.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.DE_crossover","category":"page"},{"location":"api/#Metaheuristics.DE_crossover","page":"API References","title":"Metaheuristics.DE_crossover","text":"DE_crossover(x, u, CR)\n\nBinomial crossover between x and u for Differential Evolution with probability CR, i.e., v[j] = u[j] if rand() < CR, otherwise v[j] = x[j]. Return v.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.polynomial_mutation!","category":"page"},{"location":"api/#Metaheuristics.polynomial_mutation!","page":"API References","title":"Metaheuristics.polynomial_mutation!","text":"polynomial_mutation!(vector, bounds, η=20, prob = 1 / length(vector))\n\nPolynomial Mutation applied to a vector of real numbers.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.DE_mutation","category":"page"},{"location":"api/#Metaheuristics.DE_mutation","page":"API References","title":"Metaheuristics.DE_mutation","text":"DE_mutation(population, F = 1.0, strategy = :rand1)\n\nGenerate a Vector computed from population used in Differential Evolution. Parameters: F is the stepsize, strategy can be one the following :best1, :rand2, :randToBest1 or :best2.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.MOEAD_DE_reproduction","category":"page"},{"location":"api/#Metaheuristics.MOEAD_DE_reproduction","page":"API References","title":"Metaheuristics.MOEAD_DE_reproduction","text":"MOEAD_DE_reproduction(a, b, c, F, CR, p_m, η, bounds)\n\nPerform Differential Evolution operators and polynomial mutation using three vectors a, b, c and parameters F, CR, p_m, η, i.e., stepsize, crossover and mutation probability.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.binary_tournament","category":"page"},{"location":"api/#Metaheuristics.binary_tournament","page":"API References","title":"Metaheuristics.binary_tournament","text":"binary_tournament(population)\n\nApply binary tournament to obtain a solution from from population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.GA_reproduction","category":"page"},{"location":"api/#Metaheuristics.GA_reproduction","page":"API References","title":"Metaheuristics.GA_reproduction","text":"GA_reproduction(pa::AbstractVector{T},\n                pb::AbstractVector{T},\n                bounds;\n                η_cr = 20,\n                η_m  = 15,\n                p_cr = 0.9,\n                p_m  = 0.1)\n\nCrate two solutions by applying SBX to parents pa and pb and polynomial mutation to offspring. Return two vectors.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.GA_reproduction_half","category":"page"},{"location":"api/#Metaheuristics.GA_reproduction_half","page":"API References","title":"Metaheuristics.GA_reproduction_half","text":"GA_reproduction_half(pa::AbstractVector{T},\n                pb::AbstractVector{T},\n                bounds;\n                η_cr = 20,\n                η_m  = 15,\n                p_cr = 0.9,\n                p_m  = 0.1)\n\nSame that GA_reproduction but only returns one offspring.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"RandomBinary","category":"page"},{"location":"api/#Metaheuristics.RandomBinary","page":"API References","title":"Metaheuristics.RandomBinary","text":"RandomBinary(;N)\n\nCreate random binary individuals.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"RandomPermutation","category":"page"},{"location":"api/#Metaheuristics.RandomPermutation","page":"API References","title":"Metaheuristics.RandomPermutation","text":"RandomPermutation(;N)\n\nCreate individuals in random permutations.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"TournamentSelection","category":"page"},{"location":"api/#Metaheuristics.TournamentSelection","page":"API References","title":"Metaheuristics.TournamentSelection","text":"TournamentSelection(;K=2, N=0)\n\nPerform the K-tournament selection and return N elements.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"RouletteWheelSelection","category":"page"},{"location":"api/#Metaheuristics.RouletteWheelSelection","page":"API References","title":"Metaheuristics.RouletteWheelSelection","text":"RouletteWheelSelection(;N=0)\n\nPerform Roulette Wheel Selection and return N elements.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"UniformCrossover","category":"page"},{"location":"api/#Metaheuristics.UniformCrossover","page":"API References","title":"Metaheuristics.UniformCrossover","text":"UniformCrossover(;p = 0.5)\n\nUniform crossover a.k.a. Binomial crossover. Suitable for binary representation.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"OrderCrossover","category":"page"},{"location":"api/#Metaheuristics.OrderCrossover","page":"API References","title":"Metaheuristics.OrderCrossover","text":"OrderCrossover()\n\nOrder crossover for representation where order is important. Suitable for permutation representation.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"BitFlipMutation","category":"page"},{"location":"api/#Metaheuristics.BitFlipMutation","page":"API References","title":"Metaheuristics.BitFlipMutation","text":"BitFlipMutation(;p = 1e-2)\n\nFlip each bit with probability p.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"SlightMutation","category":"page"},{"location":"api/#Metaheuristics.SlightMutation","page":"API References","title":"Metaheuristics.SlightMutation","text":"SlightMutation\n\nFogel, D. B. (1988). An evolutionary approach to the traveling salesman problem. Biological Cybernetics, 60(2), 139-144.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"PolynomialMutation","category":"page"},{"location":"api/#Metaheuristics.PolynomialMutation","page":"API References","title":"Metaheuristics.PolynomialMutation","text":"PolynomialMutation(;η, p, bounds)\n\nPolynomial mutation.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"GenerationalReplacement","category":"page"},{"location":"api/#Metaheuristics.GenerationalReplacement","page":"API References","title":"Metaheuristics.GenerationalReplacement","text":"GenerationalReplacement()\n\nGenerational replacement.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"ElitistReplacement","category":"page"},{"location":"api/#Metaheuristics.ElitistReplacement","page":"API References","title":"Metaheuristics.ElitistReplacement","text":"ElitistReplacement()\n\nOffspring is inserted in population to keep the best individuals (keep population size).\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"RankAndCrowding","category":"page"},{"location":"api/#Metaheuristics.RankAndCrowding","page":"API References","title":"Metaheuristics.RankAndCrowding","text":"RankAndCrowding()\n\nPerform environmental_selection! based non-dominated ranking and crowding distance.\n\n\n\n\n\n","category":"type"},{"location":"api/#Population","page":"API References","title":"Population","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_best","category":"page"},{"location":"api/#Metaheuristics.get_best","page":"API References","title":"Metaheuristics.get_best","text":"get_best(population)\nreturn best element in population according to the `is_better` function.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.argworst","category":"page"},{"location":"api/#Metaheuristics.argworst","page":"API References","title":"Metaheuristics.argworst","text":"argworst(population)\nreturn the index of the worst element in population\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.argbest","category":"page"},{"location":"api/#Metaheuristics.argbest","page":"API References","title":"Metaheuristics.argbest","text":"argworst(population)\nreturn the index of the worst element in population\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"nadir","category":"page"},{"location":"api/#Metaheuristics.nadir","page":"API References","title":"Metaheuristics.nadir","text":"nadir(points)\n\nComputes the nadir point from a provided array of Vectors or a population or row vectors in a Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"ideal","category":"page"},{"location":"api/#Metaheuristics.ideal","page":"API References","title":"Metaheuristics.ideal","text":"ideal(points)\n\nComputes the ideal point from a provided array of Vectors or a population or row vectors in a Matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.non_dominated_sort","category":"page"},{"location":"api/#Metaheuristics.non_dominated_sort","page":"API References","title":"Metaheuristics.non_dominated_sort","text":"non_dominated_sort(population)\n\nReturn a vector of integers r containing in r[i] the rank for population[i].\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_fronts","category":"page"},{"location":"api/#Metaheuristics.get_fronts","page":"API References","title":"Metaheuristics.get_fronts","text":"get_fronts(population, computed_ranks = true)\n\nReturn each sub-front in an array. If computed_ranks == true, this method assumes that fast_non_dominated_sort!(population) has been called before.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.fast_non_dominated_sort!","category":"page"},{"location":"api/#Metaheuristics.fast_non_dominated_sort!","page":"API References","title":"Metaheuristics.fast_non_dominated_sort!","text":"fast_non_dominated_sort!(population)\n\nSort population using the fast non dominated sorting algorithm. Note that s.rank is updated for each solution s ∈ population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_non_dominated_solutions_perm","category":"page"},{"location":"api/#Metaheuristics.get_non_dominated_solutions_perm","page":"API References","title":"Metaheuristics.get_non_dominated_solutions_perm","text":"get_non_dominated_solutions_perm(population)\n\nReturn a vector of integers v such that population[v] are the non dominated solutions contained in population.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.get_non_dominated_solutions","category":"page"},{"location":"api/#Metaheuristics.get_non_dominated_solutions","page":"API References","title":"Metaheuristics.get_non_dominated_solutions","text":"get_non_dominated_solutions(population)\n\nReturn the non dominated solutions contained in population.\n\n\n\n\n\n","category":"function"},{"location":"api/#Stopping-Criteria","page":"API References","title":"Stopping Criteria","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.diff_check","category":"page"},{"location":"api/#Metaheuristics.diff_check","page":"API References","title":"Metaheuristics.diff_check","text":"diff_check(status, information, options; d = options.f_tol, p = 0.5)\n\nCheck the difference between best and worst objective function values in current population (where at least %p of solution are feasible). Return true when such difference is <= d, otherwise return false.\n\nRef. Zielinski, K., & Laur, R. (n.d.). Stopping Criteria for Differential Evolution in Constrained Single-Objective Optimization. Studies in Computational Intelligence, 111–138. doi:10.1007/978-3-540-68830-34 (https://doi.org/10.1007/978-3-540-68830-34)\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.call_limit_stop_check","category":"page"},{"location":"api/#Metaheuristics.call_limit_stop_check","page":"API References","title":"Metaheuristics.call_limit_stop_check","text":"call_limit_stop_check(status, information, options)\n\nLimit the number of function evaluations, i.e.,  return status.f_calls >= options.f_calls_limit.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.iteration_stop_check","category":"page"},{"location":"api/#Metaheuristics.iteration_stop_check","page":"API References","title":"Metaheuristics.iteration_stop_check","text":"iteration_stop_check(status, information, options)\n\nUsed to limit the number of iterations.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.time_stop_check","category":"page"},{"location":"api/#Metaheuristics.time_stop_check","page":"API References","title":"Metaheuristics.time_stop_check","text":"time_stop_check(status, information, options)\n\nUsed to limit the time (in seconds), i.e., status.overall_time >= options.time_limit.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.accuracy_stop_check","category":"page"},{"location":"api/#Metaheuristics.accuracy_stop_check","page":"API References","title":"Metaheuristics.accuracy_stop_check","text":"accuracy_stop_check(status, information, options)\n\nIf the optimum is provided, then check if the accuracy is met via abs(status.best_sol.f - information.f_optimum) < options.f_tol.\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.var_stop_check","category":"page"},{"location":"api/#Metaheuristics.var_stop_check","page":"API References","title":"Metaheuristics.var_stop_check","text":"var_stop_check(status, information, options)\n\nCheck if the variance is close to zero in objective space.\n\n\n\n\n\n","category":"function"},{"location":"api/#Sampling","page":"API References","title":"Sampling","text":"","category":"section"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.sample","category":"page"},{"location":"api/#Metaheuristics.sample","page":"API References","title":"Metaheuristics.sample","text":"sample(method, [bounds])\n\nReturn a matrix with data by rows generated by using method (real representation) in inclusive interval [0, 1]. Here, method can be LatinHypercubeSampling, Grid or RandomInBounds.\n\nExample\n\njulia> sample(LatinHypercubeSampling(10,2))\n10×2 Matrix{Float64}:\n 0.0705631  0.795046\n 0.7127     0.0443734\n 0.118018   0.114347\n 0.48839    0.903396\n 0.342403   0.470998\n 0.606461   0.275709\n 0.880482   0.89515\n 0.206142   0.321041\n 0.963978   0.527518\n 0.525742   0.600209\n\njulia> sample(LatinHypercubeSampling(10,2), [-10 -10;10 10.0])\n10×2 Matrix{Float64}:\n -7.81644   -2.34461\n  0.505902   0.749366\n  3.90738   -8.57816\n -2.05837    9.803\n  5.62434    6.82463\n -9.34437    2.72363\n  6.43987   -1.74596\n -1.3162    -4.50273\n  9.45114   -7.13632\n -4.71696    5.0381\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.RandomInBounds","category":"page"},{"location":"api/#Metaheuristics.RandomInBounds","page":"API References","title":"Metaheuristics.RandomInBounds","text":"RandomInBounds\n\nInitialize N solutions with random values in bounds. Suitable for integer and real coded problems.\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.LatinHypercubeSampling","category":"page"},{"location":"api/#Metaheuristics.LatinHypercubeSampling","page":"API References","title":"Metaheuristics.LatinHypercubeSampling","text":"LatinHypercubeSampling(nsamples, dim; iterations)\n\nCreate N solutions within a Latin Hypercube sample in bounds with dim.\n\nExample\n\njulia> sample(LatinHypercubeSampling(10,2))\n10×2 Matrix{Float64}:\n 0.0705631  0.795046\n 0.7127     0.0443734\n 0.118018   0.114347\n 0.48839    0.903396\n 0.342403   0.470998\n 0.606461   0.275709\n 0.880482   0.89515\n 0.206142   0.321041\n 0.963978   0.527518\n 0.525742   0.600209\n\njulia> sample(LatinHypercubeSampling(10,2), [-10 -10;10 10.0])\n10×2 Matrix{Float64}:\n -7.81644   -2.34461\n  0.505902   0.749366\n  3.90738   -8.57816\n -2.05837    9.803\n  5.62434    6.82463\n -9.34437    2.72363\n  6.43987   -1.74596\n -1.3162    -4.50273\n  9.45114   -7.13632\n -4.71696    5.0381\n\n\n\n\n\n","category":"type"},{"location":"api/","page":"API References","title":"API References","text":"Metaheuristics.Grid","category":"page"},{"location":"api/#Metaheuristics.Grid","page":"API References","title":"Metaheuristics.Grid","text":"Grid(npartitions, dim)\n\nParameters to generate a grid with npartitions in a space with dim dimensions.\n\nExample\n\njulia> sample(Grid(5,2))\n25×2 Matrix{Float64}:\n 0.0   0.0\n 0.25  0.0\n 0.5   0.0\n 0.75  0.0\n ⋮     \n 0.5   1.0\n 0.75  1.0\n 1.0   1.0\n\njulia> sample(Grid(5,2), [-1 -1; 1 1.])\n25×2 Matrix{Float64}:\n -1.0  -1.0\n -0.5  -1.0\n  0.0  -1.0\n  0.5  -1.0\n  ⋮    \n  0.0   1.0\n  0.5   1.0\n  1.0   1.0\n\nNote that the sample is with size npartitions^(dim).\n\n\n\n\n\n","category":"type"},{"location":"mcdm/#Multi-Criteria-Decision-Making","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision-Making","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"A set of Multi-Criteria Decision Making (MCDM) methods is available in Metaheuristics.jl.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"compat: Maximization or Minimization\nHere, minimization is always assumed.   ","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Firstly, it is recommended to read the details of the following two functions.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"decisionmaking","category":"page"},{"location":"mcdm/#Metaheuristics.decisionmaking","page":"Multi-Criteria Decision Making","title":"Metaheuristics.decisionmaking","text":"decisionmaking(fs, w, method)\n\nPerform selected method for a given fs and weight vector(s) and return the indices indicating the best alternative(s). Here, fs can be a set of non-dominated solutions (population) or a State.\n\n\n\n\n\n","category":"function"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"best_alternative","category":"page"},{"location":"mcdm/#Metaheuristics.best_alternative","page":"Multi-Criteria Decision Making","title":"Metaheuristics.best_alternative","text":"best_alternative(res, w, method)\n\nPerform McDM using results from metaheuristic and return best alternative in res.population.\n\nExample\n\njulia> f, bounds, _ = Metaheuristics.TestProblems.ZDT1();\n\njulia> res = optimize(f, bounds, NSGA2());\n\njulia> best_sol = best_alternative(res, [0.5, 0.5], TopsisMethod())\n(f = [0.32301132058506055, 0.43208538139854685], g = [0.0], h = [0.0], x = [3.230e-01, 1.919e-04, …, 1.353e-04])\n\n\n\n\n\nbest_alternative(population, w, method)\n\nPerform method and return the best alternative(s) in population.\n\n\n\n\n\n","category":"function"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Currently available methods are listed in the following table.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Method Strategies Preferences Dependency\nCompromiseProgramming S W/D \nROIArchiving M D \nArasMethod S W JMcDM\nCocosoMethod S W JMcDM\nCodasMethod S W JMcDM\nCoprasMethod S W JMcDM\nEdasMethod S W JMcDM\nElectreMethod M W JMcDM\nGreyMethod S W JMcDM\nMabacMethod S W JMcDM\nMaircaMethod S W JMcDM\nMooraMethod S W JMcDM\nSawMethod S W JMcDM\nTopsisMethod S W JMcDM\nVikorMethod S W JMcDM\nWPMMethod S W JMcDM\nWaspasMethod S W JMcDM\nMarcosMethod S W JMcDM\nROVMethod S W JMcDM","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"A Method can suggest Single (S) or Multiple (M) Strategies. Also, Methods can represent Preferences by using weight vectors (W), reference directions (D) or reference points (P).","category":"page"},{"location":"mcdm/#JMcDM","page":"Multi-Criteria Decision Making","title":"JMcDM","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"JMcDM is a package for MCDM developed by [18]. Many methods have been implemented there, and many of them have been interfaced here.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"The main method to use JMcDM within Metaheuristics is described as follows.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"mcdm(data, w, method)","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Perform selected method for a given data and weight vector w. Here, data can be a set of non-dominated solutions (population), a State or a decision Matrix.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Also, method can be selected from JMcDM package.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Supported MCDM methods:","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"ArasMethod\nCocosoMethod\nCodasMethod\nCoprasMethod\nEdasMethod\nElectreMethod\nGreyMethod\nMabacMethod\nMaircaMethod\nMooraMethod\nSawMethod\nTopsisMethod (default method)\nVikorMethod\nWPMMethod\nWaspasMethod\nMarcosMethod\nROVMethod","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"See the JMcDM documentation for more details about the methods.","category":"page"},{"location":"mcdm/#Example-1:","page":"Multi-Criteria Decision Making","title":"Example 1:","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Performing MCDM using a population.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"julia> _, _, population = Metaheuristics.TestProblems.ZDT1();\n\njulia> dm = mcdm(population, [0.5, 0.5], TopsisMethod());\n\njulia> population[dm.bestIndex]\n(f = [0.5353535353535354, 0.2683214262030523], g = [0.0], h = [0.0], x = [5.354e-01, 0.000e+00, …, 0.000e+00])","category":"page"},{"location":"mcdm/#Example-2:","page":"Multi-Criteria Decision Making","title":"Example 2:","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Performing MCDM using results from metaheuristic.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"julia> f, bounds, _ = Metaheuristics.TestProblems.ZDT1();\n\njulia> res = optimize(f, bounds, NSGA2());\n\njulia> dm = mcdm(res, [0.5, 0.5], TopsisMethod());\n\njulia> res.population[dm.bestIndex]\n(f = [0.32301132058506055, 0.43208538139854685], g = [0.0], h = [0.0], x = [3.230e-01, 1.919e-04, …, 1.353e-04])","category":"page"},{"location":"mcdm/#Selecting-best-alternative","page":"Multi-Criteria Decision Making","title":"Selecting best alternative","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"best_alternative(res, w, method)","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"Perform McDM using results from metaheuristic and return the best alternative in res.population.","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"julia> f, bounds, _ = Metaheuristics.TestProblems.ZDT1();\n\njulia> res = optimize(f, bounds, NSGA2());\n\njulia> best_sol = best_alternative(res, [0.5, 0.5], TopsisMethod())\n(f = [0.32301132058506055, 0.43208538139854685], g = [0.0], h = [0.0], x = [3.230e-01, 1.919e-04, …, 1.353e-04])","category":"page"},{"location":"mcdm/#Region-of-Interest-Archiving","page":"Multi-Criteria Decision Making","title":"Region of Interest Archiving","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"ROIArchiving uses a set of reference directions to determine the areas of interest of the Pareto Front and a set of thresholds associated with each component from the reference directions, which determine the boundaries of the area of interest being covered. See  [19].","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"(Image: Parameters for the Region of Interest Archiving method)","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"ROIArchiving","category":"page"},{"location":"mcdm/#Metaheuristics.ROIArchiving","page":"Multi-Criteria Decision Making","title":"Metaheuristics.ROIArchiving","text":"ROIArchiving(δ_w)\n\nIt can be used as a posteriori decision-making criteria applied to the Pareto front found by a metaheuristic when solved a constrained multi-objective problem.\n\nROIArchiving can handle multiple weight points and corresponding thresholds δ_w. Note 0 <= δ_w[i] <= 1 indicates the size of region of interest, e.g., δ_w[i] = 0.51 means that  you want 51% of the Pareto front solutions close to w[i].\n\nExample\n\njulia> f, bounds, pf = Metaheuristics.TestProblems.ZDT1();\n\njulia> res = optimize(f, bounds, NSGA2()); # find Pareto front\n\njulia> ws = [0.1 0.9; 0.9 0.1]; # weight points by rows.\n\njulia> # 10% of solutions closest to the corresponding w[i]\njulia> method = ROIArchiving([0.1, 0.1]); # 10% of solutions closest to the corresponding w[i].\n\njulia> subpop = best_alternative(res, ws, method)\n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀F space⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n          ┌────────────────────────────────────────┐ \n      0.9 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⢣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⢣⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠑⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠈⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f₂     │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠁⠠⠄⠀⠀⠀⠀⠀│ \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠑⠂⠀⠀│ \n          └────────────────────────────────────────┘ \n          ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀1⠀ \n        ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀f₁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\njulia> res.population # all solutions\n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀F space⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n          ┌────────────────────────────────────────┐ \n      1.1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⡃⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠘⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠈⢆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠈⠑⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠈⠢⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f₂     │⠀⠀⠀⠀⠀⠀⠈⠓⢢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠒⢄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠑⠆⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠐⠠⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠂⠢⠄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠰⠤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠁⠑⠒⠄⢀⡀⠀⠀⠀⠀⠀│ \n        0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠒⠤⠄⣀│ \n          └────────────────────────────────────────┘ \n          ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀1⠀ \n          ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀f₁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n\n\n\n\n\n","category":"type"},{"location":"mcdm/#Compromise-Programming","page":"Multi-Criteria Decision Making","title":"Compromise Programming","text":"","category":"section"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"More information about Compromise Programming can be found in [20]","category":"page"},{"location":"mcdm/","page":"Multi-Criteria Decision Making","title":"Multi-Criteria Decision Making","text":"CompromiseProgramming","category":"page"},{"location":"mcdm/#Metaheuristics.CompromiseProgramming","page":"Multi-Criteria Decision Making","title":"Metaheuristics.CompromiseProgramming","text":"CompromiseProgramming(scalarizing)\n\nPerform compromise programming by using the scalarizing function provided. Current implemented scalarizing function are\n\nWeightedSum\nTchebysheff\nAchievementScalarization.\n\nExample\n\njulia> f, bounds, pf = Metaheuristics.TestProblems.ZDT1();\n\njulia> res = optimize(f, bounds, NSGA2());\n\njulia> w = [0.5, 0.5];\n\njulia> sol = best_alternative(res, w, CompromiseProgramming(Tchebysheff()))\n(f = [0.38493217206706115, 0.38037042164979956], g = [0.0], h = [0.0], x = [3.849e-01, 7.731e-06, …, 2.362e-07])\n\njulia> sol = best_alternative(res, w, CompromiseProgramming(WeightedSum()))\n(f = [0.2546059308425166, 0.4958366970021401], g = [0.0], h = [0.0], x = [2.546e-01, 2.929e-06, …, 2.224e-07])\n\njulia> sol = best_alternative(res, w, CompromiseProgramming(AchievementScalarization()))\n(f = [0.38493217206706115, 0.38037042164979956], g = [0.0], h = [0.0], x = [3.849e-01, 7.731e-06, …, 2.362e-07])\n\njulia> idx = decisionmaking(res, w, CompromiseProgramming(Tchebysheff()))\n3\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"List of implemented metaheuristics. The algorithms were implemented based on the contributor's understanding of the algorithms detailed in the published paper.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Algorithm Objective Constraints Large Scale Batch Evaluation Structure Name\nECA Single ✅ ➖ ✅ ECA\nDE Single ✅ ➖ ✅ DE\nPSO Single ✅ ➖ ✅ PSO\nABC Single ❌ ➖ ❌ ABC\nMOEA/D-DE Multi ➖ ➖ ❌ MOEAD_DE\nGSA Single ❌ ❌ ✅ CGSA\nSA Single ✅ ➖ ❌ SA\nWOA Single ✅ ➖ ✅ WOA\nNSGA-II Multi ✅ ➖ ✅ NSGA2\nNSGA-III Many ✅ ➖ ✅ NSGA3\nSMS-EMOA Multi ✅ ➖ ✅ SMS_EMOA\nSPEA2 Multi ✅ ➖ ✅ SPEA2\nBCA Bilevel ✅ ❌ ❌ BCA\nMCCGA Single ❌ ❌ ❌ MCCGA\nGA Single ✅ ➖ ✅ GA\nCCMO Multi ✅ ➖ ✅ CCMO\nvarepsilonDE Single ✅ ➖ ✅ εDE\nBRKGA Single ✅ ➖ ✅ BRKGA","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"✅ = supported, ❌ = not supported, ➖ = can be supported by changing default parameters.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Batch Evaluation = Simultaneous evaluation of multiple solutions (batch) see \"Batch Evaluation\".\nConstraints = Equality and inequality constraints.\nLarge Scale = High dimensional problems (variables space).","category":"page"},{"location":"algorithms/#Evolutionary-Centers-Algorithm","page":"Algorithms","title":"Evolutionary Centers Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ECA was proposed for solving global optimization problems. See [1] for more information.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ECA","category":"page"},{"location":"algorithms/#Metaheuristics.ECA","page":"Algorithms","title":"Metaheuristics.ECA","text":"ECA(;\n    η_max = 2.0,\n    K = 7,\n    N = 0,\n    N_init = N,\n    p_exploit = 0.95,\n    p_bin = 0.02,\n    p_cr = Float64[],\n    adaptive = false,\n    resize_population = false,\n    information = Information(),\n    options = Options()\n)\n\nParameters for the metaheuristic ECA: step-size η_max,K is number of vectors to generate the center of mass, N is the population size.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ECA())\n+=========== RESULT ==========+\n  iteration: 1429\n    minimum: 3.3152400000000004e-223\n  minimizer: [4.213750597785841e-113, 5.290977430907081e-112, 2.231685329262638e-112]\n    f calls: 29989\n total time: 0.1672 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ECA(N = 10, η_max = 1.0, K = 3))\n+=========== RESULT ==========+\n  iteration: 3000\n    minimum: 0.000571319\n  minimizer: [-0.00017150889316537758, -0.007955828028420616, 0.022538733289139145]\n    f calls: 30000\n total time: 0.1334 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Differential-Evolution","page":"Algorithms","title":"Differential Evolution","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"DE is an evolutionary algorithm based on vector differences. See [2] for more details.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"DE","category":"page"},{"location":"algorithms/#Metaheuristics.DE","page":"Algorithms","title":"Metaheuristics.DE","text":"DE(;\n    N  = 0,\n    F  = 1.0,\n    CR = 0.5,\n    strategy = :rand1,\n    information = Information(),\n    options = Options()\n)\n\nParameters for Differential Evolution (DE) algorithm: step-size F,CR controlls the binomial crossover, N is the population size. The parameter strategy is related to the variation operator (:rand1, :rand2, :best1, :best2, :randToBest1).\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], DE())\n+=========== RESULT ==========+\n  iteration: 1000\n    minimum: 0\n  minimizer: [0.0, 0.0, 0.0]\n    f calls: 30000\n total time: 0.0437 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], DE(N=50, F=1.5, CR=0.8))\n+=========== RESULT ==========+\n  iteration: 600\n    minimum: 8.68798e-25\n  minimizer: [3.2777877981303293e-13, 3.7650459509488005e-13, -7.871487597385812e-13]\n    f calls: 30000\n total time: 0.0319 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Particle-Swarm-Optimization","page":"Algorithms","title":"Particle Swarm Optimization","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"PSO is a population-based optimization technique inspired by the motion of bird flocks and schooling fish by [3].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"PSO","category":"page"},{"location":"algorithms/#Metaheuristics.PSO","page":"Algorithms","title":"Metaheuristics.PSO","text":"PSO(;\n    N  = 0,\n    C1 = 2.0,\n    C2 = 2.0,\n    ω  = 0.8,\n    information = Information(),\n    options = Options()\n)\n\nParameters for Particle Swarm Optimization (PSO) algorithm: learning rates C1 and C2, N is the population size and ω controls the inertia weight. \n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], PSO())\n+=========== RESULT ==========+\n  iteration: 1000\n    minimum: 1.40522e-49\n  minimizer: [3.0325415595139883e-25, 1.9862212295897505e-25, 9.543772256546461e-26]\n    f calls: 30000\n total time: 0.1558 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], PSO(N = 100, C1=1.5, C2=1.5, ω = 0.7))\n+=========== RESULT ==========+\n  iteration: 300\n    minimum: 2.46164e-39\n  minimizer: [-3.055334698085433e-20, -8.666986835846171e-21, -3.8118413472544027e-20]\n    f calls: 30000\n total time: 0.1365 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Artificial-Bee-Colony","page":"Algorithms","title":"Artificial Bee Colony","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A powerful and efficient algorithm for numerical function optimization: artificial bee colony (ABC) algorithm by [4].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"ABC","category":"page"},{"location":"algorithms/#Metaheuristics.ABC","page":"Algorithms","title":"Metaheuristics.ABC","text":"ABC(;\n    N = 50,\n    Ne = div(N+1, 2),\n    No = div(N+1, 2),\n    limit=10,\n    information = Information(),\n    options = Options()\n)\n\nABC implements the original parameters for the Artificial Bee Colony Algorithm. N is the population size, Ne is the number of employees, No is the number of outlookers bees. limit is related to the times that a solution is visited.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ABC())\n+=========== RESULT ==========+\n  iteration: 595\n    minimum: 4.03152e-28\n  minimizer: [1.489845115451046e-14, 1.2207275971717747e-14, -5.671872444705246e-15]\n    f calls: 30020\n total time: 0.0360 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], ABC(N = 80,  No = 20, Ne = 50, limit=5))\n+=========== RESULT ==========+\n  iteration: 407\n    minimum: 8.94719e-08\n  minimizer: [8.257485723496422e-5, 0.0002852795196258074, -3.5620824723352315e-5]\n    f calls: 30039\n total time: 0.0432 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#MOEA/D-DE","page":"Algorithms","title":"MOEA/D-DE","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Multiobjective optimization problems with complicated Pareto sets by [5].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"MOEAD_DE","category":"page"},{"location":"algorithms/#Metaheuristics.MOEAD_DE","page":"Algorithms","title":"Metaheuristics.MOEAD_DE","text":"MOEAD_DE(weights ;\n    F = 0.5,\n    CR = 1.0,\n    λ = Array{Vector{Float64}}[], # ref. points\n    η = 20,\n    p_m = -1.0,\n    T = round(Int, 0.2*length(weights)),\n    δ = 0.9,\n    n_r = round(Int, 0.02*length(weights)),\n    z = zeros(0),\n    B = Array{Int}[],\n    s1 = 0.01,\n    s2 = 20.0,\n    information = Information(),\n    options = Options())\n\nMOEAD_DE implements the original version of MOEA/D-DE. It uses the contraint handling method based on the sum of violations (for constrained optimizaton): g(x, λ, z) = max(λ .* abs.(fx - z)) + sum(max.(0, gx)) + sum(abs.(hx))\n\nTo use MOEAD_DE, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are the equality and inequality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nRef. Multiobjective Optimization Problems With Complicated Pareto Sets, MOEA/D and NSGA-II; Hui Li and Qingfu Zhang.\n\nExample\n\nAssume you want to solve the following optimizaton problem:\n\nMinimize:\n\nf(x) = (x_1, x_2)\n\nsubject to:\n\ng(x) = x_1^2 + x_2^2 - 1 ≤ 0\n\nx_1, x_2 ∈ [-1, 1]\n\nA solution can be:\n\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] )\n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\nnobjectives = 2\nnpartitions = 100\n\n# reference points (Das and Dennis's method)\nweights = gen_ref_dirs(nobjectives, npartitions)\n\n# define the parameters\nmoead_de = MOEAD_DE(weights, options=Options(debug=false, iterations = 250))\n\n# optimize\nstatus_moead = optimize(f, bounds, moead_de)\n\n# show results\ndisplay(status_moead)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Gravitational-Search-Algorithm","page":"Algorithms","title":"Gravitational Search Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Chaotic gravitational constants for the gravitational search algorithm by [6]","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"CGSA","category":"page"},{"location":"algorithms/#Metaheuristics.CGSA","page":"Algorithms","title":"Metaheuristics.CGSA","text":"CGSA(;\n    N::Int    = 30,\n    chValueInitial::Real   = 20,\n    chaosIndex::Real   = 9,\n    ElitistCheck::Int    = 1,\n    Rpower::Int    = 1,\n    Rnorm::Int    = 2,\n    wMax::Real   = chValueInitial,\n    wMin::Real   = 1e-10,\n    information = Information(),\n    options = Options()\n)\n\nCGSA is an extension of the GSA algorithm but with Chaotic gravitational constants for the gravitational search algorithm.\n\nRef. Chaotic gravitational constants for the gravitational search algorithm. Applied Soft Computing 53 (2017): 407-419.\n\nParameters:\n\nN: Population size\nchValueInitial: Initial value for the chaos value\nchaosIndex: Integer 1 ≤ chaosIndex ≤ 10 is the function that model the chaos\nRpower: power related to the distance norm(x)^Rpower\nRnorm: is the value as in norm(x, Rnorm)\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], CGSA())\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 8.63808e-08\n  minimizer: [0.0002658098418993323, -1.140808975532608e-5, -0.00012488307670533095]\n    f calls: 15000\n total time: 0.1556 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], CGSA(N = 80, chaosIndex = 1))\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 1.0153e-09\n  minimizer: [-8.8507563788141e-6, -1.3050111801923072e-5, 2.7688577445980026e-5]\n    f calls: 40000\n total time: 1.0323 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Simulated-Annealing","page":"Algorithms","title":"Simulated Annealing","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Physics-inspired algorithm for optimization by [7].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SA","category":"page"},{"location":"algorithms/#Metaheuristics.SA","page":"Algorithms","title":"Metaheuristics.SA","text":"    SA(;\n        x_initial::Vector = zeros(0),\n        N::Int = 500,\n        tol_fun::Real= 1e-4,\n        information = Information(),\n        options = Options()\n    )\n\nParameters for the method of Simulated Annealing (Kirkpatrick et al., 1983).\n\nParameters:\n\nx_intial: Inital solution. If empty, then SA will generate a random one within the bounds.\nN: The number of test points per iteration.\ntol_fun: tolerance value for the Metropolis condition to accept or reject the test point as current point.\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], SA())\n+=========== RESULT ==========+\n  iteration: 60\n    minimum: 5.0787e-68\n  minimizer: [-2.2522059499734615e-34, 3.816133503985569e-36, 6.934348004465088e-36]\n    f calls: 29002\n total time: 0.0943 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], SA(N = 100, x_initial = [1, 0.5, -1]))\n+=========== RESULT ==========+\n  iteration: 300\n    minimum: 1.99651e-69\n  minimizer: [4.4638292404181215e-35, -1.738939846089388e-36, -9.542441152683457e-37]\n    f calls: 29802\n total time: 0.0965 s\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#Whale-Optimization-Algorithm","page":"Algorithms","title":"Whale Optimization Algorithm","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"The Whale Optimization Algorithm inspired by humpback whales proposed in [8].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"WOA","category":"page"},{"location":"algorithms/#Metaheuristics.WOA","page":"Algorithms","title":"Metaheuristics.WOA","text":"WOA(;N = 30, information = Information(), options = Options())\n\nParameters for the Whale Optimization Algorithm. N is the population size (number of whales).\n\nExample\n\njulia> f(x) = sum(x.^2)\nf (generic function with 1 method)\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], WOA())\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 3.9154600000000003e-100\n  minimizer: [8.96670478694908e-52, -1.9291317455298046e-50, 4.3113080446722046e-51]\n    f calls: 15000\n total time: 0.0134 s\n+============================+\n\njulia> optimize(f, [-1 -1 -1; 1 1 1.0], WOA(N = 100))\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 1.41908e-145\n  minimizer: [9.236161414012512e-74, -3.634919950380001e-73, 3.536831799149254e-74]\n    f calls: 50000\n total time: 0.0588 s\n+============================+\n\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NSGA-II","page":"Algorithms","title":"NSGA-II","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A fast and elitist multiobjective genetic algorithm: NSGA-II by [9].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"NSGA2","category":"page"},{"location":"algorithms/#Metaheuristics.NSGA2","page":"Algorithms","title":"Metaheuristics.NSGA2","text":"NSGA2(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-II.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\n\nTo use NSGA2, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `NSGA2()` for using default parameters)\nnsga2 = NSGA2(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, nsga2)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#NSGA-III","page":"Algorithms","title":"NSGA-III","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"An Evolutionary Many-Objective Optimization Algorithm Using Reference-Point-Based Nondominated Sorting Approach, Part I: Solving Problems With Box Constraints by [10].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"NSGA3","category":"page"},{"location":"algorithms/#Metaheuristics.NSGA3","page":"Algorithms","title":"Metaheuristics.NSGA3","text":"NSGA3(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    partitions = 12,\n    reference_points = Vector{Float64}[],\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-III.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\nreference_points reference points usually generated by gen_ref_dirs.\npartitions number of Das and Dennis's reference points if reference_points is empty.\n\nTo use NSGA3, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n\n# Objective function, bounds, and the True Pareto front\nf, bounds, pf = Metaheuristics.TestProblems.get_problem(:DTLZ2)\n\n\n# define the parameters (use `NSGA3()` for using default parameters)\nnsga3 = NSGA3(p_cr = 0.9)\n\n# optimize\nstatus = optimize(f, bounds, nsga3)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#SMS-EMOA","page":"Algorithms","title":"SMS-EMOA","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"An EMO algorithm using the hypervolume measure as a selection criterion by [11].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SMS_EMOA","category":"page"},{"location":"algorithms/#Metaheuristics.SMS_EMOA","page":"Algorithms","title":"Metaheuristics.SMS_EMOA","text":"SMS_EMOA(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    n_samples = 10_000,\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic SMS-EMOA.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\nn_samples number of samples to approximate hypervolume in many-objective (M > 2).\n\nTo use SMS_EMOA, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `SMS_EMOA()` for using default parameters)\nsms_emoa = SMS_EMOA(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, sms_emoa)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#SPEA2","page":"Algorithms","title":"SPEA2","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Improved strength Pareto evolutionary algorithm by [12].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"SPEA2","category":"page"},{"location":"algorithms/#Metaheuristics.SPEA2","page":"Algorithms","title":"Metaheuristics.SPEA2","text":"SPEA2(;\n    N = 100,\n    η_cr = 20,\n    p_cr = 0.9,\n    η_m = 20,\n    p_m = 1.0 / D,\n    information = Information(),\n    options = Options(),\n)\n\nParameters for the metaheuristic NSGA-II.\n\nParameters:\n\nN Population size.\nη_cr  η for the crossover.\np_cr Crossover probability.\nη_m  η for the mutation operator.\np_m Mutation probability (1/D for D-dimensional problem by default).\n\nTo use SPEA2, the output from the objective function should be a 3-touple (f::Vector, g::Vector, h::Vector), where f contains the objective functions, g and h are inequality, equality constraints respectively.\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nusing Metaheuristics\n\n# Dimension\nD = 2\n\n# Objective function\nf(x) = ( x, [sum(x.^2) - 1], [0.0] ) \n\n# bounds\nbounds = [-1 -1;\n           1  1.0\n        ]\n\n# define the parameters (use `SPEA2()` for using default parameters)\nnsga2 = SPEA2(N = 100, p_cr = 0.85)\n\n# optimize\nstatus = optimize(f, bounds, nsga2)\n\n# show results\ndisplay(status)\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#BCA","page":"Algorithms","title":"BCA","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Bilevel Centers Algorithm has been proposed to solve bilevel optimization problems. See BilevelHeuristics.BCA for  details.","category":"page"},{"location":"algorithms/#MCCGA","page":"Algorithms","title":"MCCGA","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Machine-coded Compact Genetic Algorithms for real-valued optimization problems by [13].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"MCCGA","category":"page"},{"location":"algorithms/#Metaheuristics.MCCGA","page":"Algorithms","title":"Metaheuristics.MCCGA","text":"MCCGA(;N, maxsamples)\n\nParameters:\n\nN population size. Default is 100.\nmaxsamples maximum number of samples. Default is 10000.\n\nDescription\n\nMCCGA method implements the Machine-coded genetic algorithms for real valued optimization problems.  The algorithm is based on the concept of a compact genetic algorithm but with a machine-coded representation  using IEEE-754 floating point encoding standard. In the first stage of the algorithm, maxsamples number  of samples are generated within the range of function domain. This process is required to obtain a vector of probabilities for each single bit of the IEEE-754 representation. In classical CGAs, the initial vector  of probabilities is generated using the constant probability of 0.5 whereas in MCCGA, the probability of ith bit having a value of 1 depends on the function domain. The second step performs a CGA search but with IEEE-754 bits again. Since  CGA does not use a population of solutions but a single vector of probabilities, the parameter N does not really mean  number of solutions. Instead, it means the amount of mutation in each iteration, e.g. 1/N.  In the last stage, a local search is performed for fine-tuning. In this implementation, Hooke & Jeeves  direct search algorithm is used.\n\nReferences\n\nMoser, Irene. \"Hooke-Jeeves Revisited.\" 2009 IEEE Congress on Evolutionary Computation. IEEE, 2009.\nHarik, G. R., Lobo, F. G., & Goldberg, D. E. (1999). The compact genetic algorithm. IEEE transactions on evolutionary computation, 3(4), 287-297.\nSatman, M. H. & Akadal, E. (2020). Machine Coded Compact Genetic Algorithms for Real Parameter Optimization Problems . Alphanumeric Journal , 8 (1) , 43-58 . DOI: 10.17093/alphanumeric.576919\nMehmet Hakan Satman, Emre Akadal, Makine Kodlu Hibrit Kompakt Genetik Algoritmalar Optimizasyon Yöntemi, Patent, TR, 2022/01, 2018-GE-510239\n\nExample\n\n\njulia> f, bounds, solutions = Metaheuristics.TestProblems.rastrigin();\n\njulia> result = optimize(f, bounds, MCCGA())\n\n+=========== RESULT ==========+\n  iteration: 42833\n    minimum: 0\n  minimizer: [-5.677669786379456e-17, 2.7942451898022582e-39, -3.60925916059986e-33, -6.609510861086017e-34, 2.998586759675011e-32, -1.8825832500775007e-38, -3.0729484147585938e-31, 1.7675578057632446e-38, 5.127944874215823e-16, 1.9623770480857484e-19]\n    f calls: 86404\n total time: 3.2839 s\n+============================+\n\nExplicit Example\n\n\njulia> using Metaheuristics                                                                                         \n                                                              \njulia> f(x::Vector{Float64})::Float64 = (x[1]-pi)^2 + (x[2]-exp(1))^2                                                                                \n                                                              \njulia> bounds = [ -500.0  -500.0;                                    \n                   500.0  500.0]                                      \n\njulia> result = Metaheuristics.optimize(f, bounds, MCCGA())\n\n+=========== RESULT ==========+\n  iteration: 2974\n    minimum: 1.80997e-09\n  minimizer: [3.1416284249228976, 2.7183048585824263]\n    f calls: 6012\n total time: 1.5233 s\nstop reason: Other stopping criteria.\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#GA","page":"Algorithms","title":"GA","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"GA","category":"page"},{"location":"algorithms/#Metaheuristics.GA","page":"Algorithms","title":"Metaheuristics.GA","text":"GA(;\n    N = 100,\n    p_mutation  = 1e-5,\n    p_crossover = 0.5,\n    initializer = RandomInBounds(),\n    selection   = TournamentSelection(),\n    crossover   = UniformCrossover(),\n    mutation    = BitFlipMutation(),\n    environmental_selection = ElitistReplacement()\n    )\n\nJust another Genetic Algorithm Framework.\n\nParameters:\n\nN is the population size.\np_mutation  mutation probability (gen).\np_crossover crossover probability (gen).\ninitializer parameters for the population initializer.\nselection parameters for the selection operator.\ncrossover parameters for the crossover operators. \nmutation parameters for the mutation operator.\nenvironmental_selection parameters for the replacement method.\n\nExample: Binary Encoding (default)\n\njulia> f(x) = sum(x) / length(x)\nf (generic function with 1 method)\n\njulia> dim = 10;\n\njulia> optimize(f, BitArraySpace(dim), GA())\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 0\n  minimizer: Bool[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    f calls: 50000\n total time: 0.0523 s\nstop reason: Maximum number of iterations exceeded.\n+============================+\n\nExample: Permutation Encoding\n\njulia> f(x) = sum(abs.(x .- (length(x):-1:1.0)))\nf (generic function with 1 method)\n\njulia> perm_size = 10;\n\njulia> ga = GA(;initializer = RandomPermutation(N=100), crossover=OrderCrossover(), mutation=SlightMutation());\n\njulia> optimize(f, PermutationSpace(perm_size), ga)\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 0\n  minimizer: [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n    f calls: 49900\n total time: 0.6230 s\nstop reason: Maximum number of iterations exceeded.\n+============================+\n\nExample: Integer Encoding\n\njulia> c = randn(10);\n\njulia> A = randn(10, 10);\n\njulia> f(x) = c'*x, A*x, [0.0]\nf (generic function with 1 method)\n\njulia> bounds = repeat([0,100], 1, 10)\n2×10 Matrix{Int64}:\n   0    0    0    0    0    0    0    0    0    0\n 100  100  100  100  100  100  100  100  100  100\n\njulia> ga = GA(;crossover=SBX(;bounds),\n                mutation=PolynomialMutation(;bounds),\n                environmental_selection=GenerationalReplacement()\n            );\n\njulia> result = optimize(f, bounds, ga)\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: -76.9031\n  minimizer: [0, 10, 49, 100, 24, 0, 0, 0, 67, 76]\n    f calls: 50000\n  feasibles: 95 / 100 in final population\n total time: 0.6028 s\nstop reason: Maximum number of iterations exceeded.\n+============================+\n\nExample: Real Encoding\n\njulia> f, bounds, solutions = Metaheuristics.TestProblems.rastrigin();\n\njulia> ga = GA(;mutation=PolynomialMutation(;bounds),\n                crossover=SBX(;bounds),\n                environmental_selection=GenerationalReplacement()\n               );\n\njulia> optimize(f, bounds, ga)\n+=========== RESULT ==========+\n  iteration: 500\n    minimum: 5.91136e-09\n  minimizer: [-5.446073166732363e-6, -1.7900876625850504e-7, 2.8548505431323723e-8, 2.1130514595980084e-8, -1.632381298278562e-8, 2.8216219650016283e-9, -3.2114953600427333e-7, 2.4222648522114125e-9, -5.5236545829928716e-9, -2.3479408274628334e-9]\n    f calls: 49900\n total time: 0.5775 s\nstop reason: Maximum number of iterations exceeded.\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#CCMO","page":"Algorithms","title":"CCMO","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"A Coevolutionary Framework for Constrained Multiobjective Optimization Problems proposed by [14].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"CCMO","category":"page"},{"location":"algorithms/#Metaheuristics.CCMO","page":"Algorithms","title":"Metaheuristics.CCMO","text":"CCMO(base_optimizer; infromation, options)\n\nParameters for CCMO algorithm. base_algorithm only supports NSGA2().\n\nA feasible solution is such that g_i(x) ≤ 0 and h_j(x) = 0.\n\nExample\n\njulia> f, bounds, pf = Metaheuristics.TestProblems.MTP();\n\njulia> ccmo = CCMO(NSGA2(N=100, p_m=0.001));\n\njulia> optimize(f, bounds, ccmo)\n+=========== RESULT ==========+\n  iteration: 500\n population:        ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀F space⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n        ┌────────────────────────────────────────┐ \n      2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f₂   │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⢧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠈⢢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠈⠢⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠈⠑⠦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⠤⢤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠒⠢⠤⡀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n      0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠁⠒⠒⠒⠄⠤⠤⢠⢀⣀⢀⣀⠀⣀⣀⡀⣀⡀│ \n        └────────────────────────────────────────┘ \n        ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀1⠀ \n        ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀f₁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \nnon-dominated solution(s):\n        ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀F space⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n        ┌────────────────────────────────────────┐ \n      2 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f₂   │⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⢧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠈⢢⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠈⠢⢀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠈⠑⠦⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠉⠒⠤⢤⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n        │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠑⠒⠢⠤⡀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n      0 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠁⠒⠒⠒⠄⠤⠤⢠⢀⣀⢀⣀⠀⣀⣀⡀⣀⡀│ \n        └────────────────────────────────────────┘ \n        ⠀0⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀1⠀ \n        ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀f₁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ \n    f calls: 50000\n  feasibles: 100 / 100 in final population\n total time: 7.0616 s\nstop reason: Maximum number of iterations exceeded.\n+============================+\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#\\varepsilonDE","page":"Algorithms","title":"varepsilonDE","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"varepsilon Constrained Differential Evolution with Gradient-Based Mutation and Feasible Elites by [15].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"warning: Gradient mutation\nGradient mutation is not implemented here.","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"εDE","category":"page"},{"location":"algorithms/#Metaheuristics.εDE","page":"Algorithms","title":"Metaheuristics.εDE","text":"εDE(cp = 5, DE_kargs...)\nepsilonDE(cp = 5, DE_kargs...)\n\nParameters for ε Differential Evolution for constrained optimization.\n\nSee DE for more details about DE parameters (DE_kargs).\n\nThis implementation is not implementing the gradient-based repair method.\n\n\n\n\n\n","category":"type"},{"location":"algorithms/#BRKGA","page":"Algorithms","title":"BRKGA","text":"","category":"section"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"Biased Random Key Genetic Algorithm by [16].","category":"page"},{"location":"algorithms/","page":"Algorithms","title":"Algorithms","text":"BRKGA","category":"page"},{"location":"algorithms/#Metaheuristics.BRKGA","page":"Algorithms","title":"Metaheuristics.BRKGA","text":"BRKGA(num_elites = 20, num_mutants = 10, num_offsprings = 70, bias = 0.7)\n\nBiased Random Key Genetic Algorithm (BRKGA).\n\nExample\n\njulia> target_perm = collect(reverse(1:10))\n10-element Vector{Int64}:\n 10\n  9\n  8\n  7\n  6\n  5\n  4\n  3\n  2\n  1\n\njulia> decode(rk) = sortperm(rk);\n\njulia> f(rk) = sum(abs.(decode(rk) - target_perm));\n\njulia> res = optimize(f, [zeros(10) ones(10)], BRKGA(num_elites=70))\nOptimization Result\n===================\n  Iteration:       22\n  Minimum:         0\n  Minimizer:       [0.938595, 0.851247, 0.823736, …, 0.375321]\n  Function calls:  2200\n  Total time:      0.0238 s\n  Stop reason:     Due to Convergence Termination criterion.\n\njulia> decode(minimizer(res))\n10-element Vector{Int64}:\n 10\n  9\n  8\n  7\n  6\n  5\n  4\n  3\n  2\n  1\n\n\n\n\n\n","category":"function"},{"location":"problems/#Problems","page":"Problems","title":"Problems","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":"Benchmark Test Problems for numerical optimization.","category":"page"},{"location":"problems/","page":"Problems","title":"Problems","text":" Metaheuristics.TestProblems.get_problem","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.get_problem","page":"Problems","title":"Metaheuristics.TestProblems.get_problem","text":"get_problem(problem)\n\nReturns a 3-tuple with the objective function, the bounds and 100 Pareto solutions for multi-objective optimization problems or the optimal solutions for (box)constrained optimization problems.\n\nHere, problem can be one of the following symbols:\n\nSingle-objective:\n\n:sphere\n:discus\n:rastrigin\n\nMulti-objective:\n\n:ZDT1\n:ZDT2\n:ZDT3\n:ZDT4\n:ZDT6\n\nMany-objective:\n\n:DTLZ1\n:DTLZ2\n:DTLZ3\n:DTLZ4\n:DTLZ5\n:DTLZ6\n\nExample\n\njulia> import Metaheuristics: TestProblems, optimize\n\njulia> f, bounds, pareto_solutions = TestProblems.get_problem(:ZDT3);\n\n\njulia> bounds\n2×30 Array{Float64,2}:\n 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0\n\njulia> pareto_solutions\n                           F space\n          ┌────────────────────────────────────────┐ \n        1 │⢅⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠈⢢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠙⠒⠀⠀⠀⠀⡆⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠘⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠱⠄⠀⠀⠀⠀⠀⠀⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢱⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n   f_2    │⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠬⡦⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤⠤│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠂⠀⠀⠀⠀⠀⠀⢢⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢧⡀⠀⠀⠀⠀⠀⠀⢀⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⠀⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⡆⠀⠀│ \n          │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠀⠀│ \n       -1 │⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀│ \n          └────────────────────────────────────────┘ \n          0                                      0.9\n                             f_1\n\n\n\n\n\n\n","category":"function"},{"location":"problems/#Box-constrained-Optimization","page":"Problems","title":"Box-constrained Optimization","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":" Metaheuristics.TestProblems.sphere","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.sphere","page":"Problems","title":"Metaheuristics.TestProblems.sphere","text":"sphere(D)\n\nThe well-known D-dimensional Sphere function.\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.discus","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.discus","page":"Problems","title":"Metaheuristics.TestProblems.discus","text":"discus(D)\n\nThe well-known D-dimensional Discus function.\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.rastrigin","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.rastrigin","page":"Problems","title":"Metaheuristics.TestProblems.rastrigin","text":"rastrigin(D)\n\nThe well-known D-dimensional Rastrigin function.\n\n\n\n\n\n","category":"function"},{"location":"problems/#Constrained-Optimization","page":"Problems","title":"Constrained Optimization","text":"","category":"section"},{"location":"problems/#Multi-objective-Optimization","page":"Problems","title":"Multi-objective Optimization","text":"","category":"section"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT1","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT1","page":"Problems","title":"Metaheuristics.TestProblems.ZDT1","text":"ZDT1(D, n_solutions)\n\nZDT1 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT2","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT2","page":"Problems","title":"Metaheuristics.TestProblems.ZDT2","text":"ZDT2(D, n_solutions)\n\nZDT2 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nnonconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT3","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT3","page":"Problems","title":"Metaheuristics.TestProblems.ZDT3","text":"ZDT3(D, n_solutions)\n\nZDT3 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nconvex disconected\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT4","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT4","page":"Problems","title":"Metaheuristics.TestProblems.ZDT4","text":"ZDT4(D, n_solutions)\n\nZDT4 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of pareto solutions.\n\nMain properties:\n\nnonconvex\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.ZDT6","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.ZDT6","page":"Problems","title":"Metaheuristics.TestProblems.ZDT6","text":"ZDT6(D, n_solutions)\n\nZDT6 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nD number of variables (dimension)\nn_solutions number of Pareto solutions.\n\nMain properties:\n\nnonconvex\nnon-uniformly spaced\n\n\n\n\n\n","category":"function"},{"location":"problems/","page":"Problems","title":"Problems","text":"Metaheuristics.TestProblems.DTLZ2","category":"page"},{"location":"problems/#Metaheuristics.TestProblems.DTLZ2","page":"Problems","title":"Metaheuristics.TestProblems.DTLZ2","text":"DTLZ2(m = 3, ref_dirs = gen_ref_dirs(m, 12))\n\nDTLZ2 returns (f::function, bounds::Matrix{Float64}, pareto_set::Array{xFgh_indiv}) where f is the objective function and pareto_set is an array with optimal Pareto solutions with n_solutions.\n\nParameters\n\nm number of objective functions\nref_dirs number of Pareto solutions (default: Das and Dennis' method).\n\nMain properties:\n\nnonconvex\nunifrontal\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"This package provides different tools for optimization. Hence, this section gives different examples for using the implemented Metaheuristics.","category":"page"},{"location":"examples/#Single-Objective-Optimization","page":"Examples","title":"Single-Objective Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Firstly import this package","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now, let us define the objective function to be minimized:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x) )","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The search space (a.k.a. box-constraints) can be defined as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"bounds = boxconstraints(lb = -5ones(10), ub = 5ones(10))","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"compat: boxconstraints in a Matrix format.\nYou can also define the bounds using bounds = [-5ones(10) 5ones(10)]'; however this is not longer recommended.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"It is possible to provide some information on the minimization problem. Let's provide the true optimum to stop the optimizer when a tolerance f_tol is satisfied.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"information = Information(f_optimum = 0.0)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Generic options or settings (e.g. budget limitation, tolerances, etc) can be provided as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"options = Options(f_calls_limit = 9000*10, f_tol = 1e-5, seed=1)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now, we can provide the Information and Options to the optimizer (ECA in this example).","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"algorithm = ECA(information = information, options = options)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Now, the optimization is performed as follows:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"result = optimize(f, bounds, algorithm)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The minimum and minimizer:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"minimum(result)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"minimizer(result)","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"compat: Second run is faster in Julia\nAs you may know, the second run can be faster.","category":"page"},{"location":"examples/#Constrained-Optimization","page":"Examples","title":"Constrained Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"It is common that optimization models include constraints that must be satisfied. For example: The Rosenbrock function constrained to a disk","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Minimize:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"displaystyle f(xy)=(1-x)^2+100(y-x^2)^2","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"subject to:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"displaystyle x^2+y^2leq 2","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"where -2 leq xy leq 2.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"In Metaheuristics.jl, a feasible solution is such that g(x) leq 0 and h(x) approx 0. Hence, in this example the constraint is given by g(x) = x^2 + y^2 - 2 leq 0. Moreover, the equality and inequality constraints must be saved into  Arrays.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"compat: Constraints handling\nIn this package, if the algorithm was not designed for constrained optimization, then solutions with the lower constraint violation sum will be preferred.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\n\nfunction f(x)\n    x,y = x[1], x[2]\n\n    fx = (1-x)^2+100(y-x^2)^2\n    gx = [x^2 + y^2 - 2] # inequality constraints\n    hx = [0.0] # equality constraints\n\n    # order is important\n    return fx, gx, hx\nend\n\nbounds = [-2.0 -2; 2 2]\n\noptimize(f, bounds, ECA(N=30, K=3))","category":"page"},{"location":"examples/#Multiobjective-Optimization","page":"Examples","title":"Multiobjective Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"To implement a multiobjective optimization problem and solve it, you can proceed as usual. Here, you need to provide constraints if they exist, otherwise put gx = [0.0]; hx = [0.0]; to indicate an unconstrained multiobjective problem.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using UnicodePlots # to visualize in console (optional)\nusing Metaheuristics\n\nfunction f(x)\n    # objective functions\n    v = 1.0 + sum(x .^ 2)\n    fx1 = x[1] * v\n    fx2 = (1 - sqrt(x[1])) * v\n\n    fx = [fx1, fx2]\n\n    # constraints\n    gx = [0.0] # inequality constraints\n    hx = [0.0] # equality constraints\n\n    # order is important\n    return fx, gx, hx\nend\n\nbounds = [zeros(30) ones(30)]';\n\noptimize(f, bounds, NSGA2())","category":"page"},{"location":"examples/#Bilevel-Optimization","page":"Examples","title":"Bilevel Optimization","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Bilevel optimization problems can be solved by using the package BilevelHeuristics.jl which extends  Metaheuristics.jl for handling those hierarchical problems.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Defining objective functions corresponding to the BO problem.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Upper level (leader problem):","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using BilevelHeuristics\n\nF(x, y) = sum(x.^2) + sum(y.^2)\nbounds_ul = [-ones(5) ones(5)] ","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Lower level (follower problem):","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"f(x, y) = sum((x - y).^2) + y[1]^2\nbounds_ll = [-ones(5) ones(5)];","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Approximate solution:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"res = optimize(F, f, bounds_ul, bounds_ll, BCA())","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Output:","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"+=========== RESULT ==========+\n  iteration: 108\n    minimum: \n          F: 4.03387e-10\n          f: 2.94824e-10\n  minimizer: \n          x: [-1.1460768817533927e-5, 7.231706879604178e-6, 3.818596951258517e-6, 2.294324313691869e-6, 1.8770952450067828e-6]\n          y: [1.998748659975197e-6, 9.479307908087866e-6, 6.180041276047425e-6, -7.642051857319683e-6, 2.434166021682429e-6]\n    F calls: 2503\n    f calls: 5062617\n    Message: Stopped due UL function evaluations limitations. \n total time: 26.8142 s\n+============================+","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"See BilevelHeuristics documentation for more information.","category":"page"},{"location":"examples/#Decision-Making","page":"Examples","title":"Decision-Making","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Although Metaheuristics is focused on the optimization part, some decision-making algorithms are available in this package (see Multi-Criteria Decision-Making).","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"The following example shows how to perform a posteriori decision-making.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"julia> # load the problem\njulia> f, bounds, pf = Metaheuristics.TestProblems.ZDT1();\n\njulia> # perform multi-objective optimization\njulia> res = optimize(f, bounds, NSGA2());\n\njulia> # user preferences\njulia> w = [0.5, 0.5];\n\njulia> # set the decision-making algorithm\njulia> dm_method = CompromiseProgramming(Tchebysheff())\n\njulia> # find the best decision\njulia> sol = best_alternative(res, w, dm_method)\n(f = [0.38493217206706115, 0.38037042164979956], g = [0.0], h = [0.0], x = [3.849e-01, 7.731e-06, …, 2.362e-07])","category":"page"},{"location":"examples/#Providing-Initial-Solutions","page":"Examples","title":"Providing Initial Solutions","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Sometimes you may need to use the starter solutions you need before the optimization process begins, well, this example illustrates how to do it.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics # hide\nf(x) = abs(x[1]) + x[2]  + x[3]^2 # objective function\nalgo  = ECA(N = 61); # optimizer\n\n# one solution can be provided\nx0 = [0.5, 0.5, 0.5];\n\nset_user_solutions!(algo, x0, f);\n\n# or multiple solutions can be given\nX0 = rand(30, 3); # 30 solutions with dim 3\n\nset_user_solutions!(algo, X0, f);\noptimize(f, [0 0 0; 1 1 1.0], algo)","category":"page"},{"location":"examples/#Batch-Evaluation","page":"Examples","title":"Batch Evaluation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Evaluating multiple solutions at the same time can reduce computational time. To do that, define your function on an input N x D matrix and function values into matrices with outcomes in rows for all N solutions. Also, you need to put parallel_evaluation=true in the Options to indicate that your f is prepared for parallel evaluations.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"f(X) = begin\n    fx = sum(X.^2, dims=2)       # objective function ∑x²\n    gx = sum(X.^2, dims=2) .-0.5 # inequality constraints ∑x² ≤ 0.5\n    hx = zeros(0,0)              # equality constraints\n    fx, gx, hx\nend\n\noptions = Options(parallel_evaluation=true)\n\nres = optimize(f, [-10ones(5) 10ones(5)], ECA(options=options))","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"See Parallelization tutorial for more details.","category":"page"},{"location":"examples/#Modifying-an-Existing-Metaheuristic","page":"Examples","title":"Modifying an Existing Metaheuristic","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"You may need to modify one of the implemented metaheuristics to improve the algorithm performance or test new mechanisms. This example illustrates how to do it.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"warning: Modifying algorithms could break stuff\nBe cautious when modifying a metaheuristic due to those changes will overwrite the default method for that metaheuristic.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"Let's assume that we want to modify the stop criteria for ECA. See Contributing  for more details.","category":"page"},{"location":"examples/","page":"Examples","title":"Examples","text":"using Metaheuristics\nimport LinearAlgebra: norm\n\n# overwrite method\nfunction Metaheuristics.stop_criteria!(\n        status,\n        parameters::ECA, # It is important to indicate the modified Metaheuristic \n        problem,\n        information,\n        options,\n        args...;\n        kargs...\n    )\n\n    if status.stop\n        # nothing to do\n        return\n    end\n\n    # Diversity-based stop criteria\n\n    x_mean = zeros(length(status.population[1].x))\n    for sol in status.population\n        x_mean += sol.x\n    end\n    x_mean /= length(status.population)\n    \n    distances_mean = sum(sol -> norm( x_mean - sol.x ), status.population)\n    distances_mean /= length(status.population)\n\n    # stop when solutions are close enough to the geometrical center\n    new_stop_condition = distances_mean <= 1e-3\n\n    status.stop = new_stop_condition\n\n    # (optional and not recommended) print when this criterium is met\n    if status.stop\n        @info \"Diversity-based stop criterium\"\n        @show distances_mean\n    end\n\n\n    return\nend\n\nf, bounds, opt = Metaheuristics.TestProblems.get_problem(:sphere);\noptimize(f, bounds, ECA())\n","category":"page"},{"location":"examples/#Restarting-search","page":"Examples","title":"Restarting search","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Restart","category":"page"},{"location":"examples/#Metaheuristics.Restart","page":"Examples","title":"Metaheuristics.Restart","text":"Restart(optimizer, every=100)\n\nResets the optimizer every specified number of iterations (100 by default).\n\nExample\n\njulia> f, bounds, _ = Metaheuristics.TestProblems.rastrigin();\n\njulia> optimize(f, bounds, Restart(ECA(), every=200))\n\nCustomization\n\nThe restart condition can be updated by overloading the restart_condition method:\n\nfunction Metaheuristics.restart_condition(status, restart::Restart, information, options)\n    st.iteration % params.every == 0\nend\n\n\n\n\n\n","category":"type"},{"location":"#Metaheuristics-an-Intuitive-Package-for-Global-Optimization","page":"Index","title":"Metaheuristics - an Intuitive Package for Global Optimization","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Author: Jesús-Adolfo Mejía-de-Dios (@jmejia8)","category":"page"},{"location":"","page":"Index","title":"Index","text":"High-performance algorithms for optimization coded purely in a high-performance language.","category":"page"},{"location":"","page":"Index","title":"Index","text":"(Image: Source) (Image: Build Status) (Image: codecov) (Image: DOI)","category":"page"},{"location":"#Introduction","page":"Index","title":"Introduction","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Optimization is one of the most common tasks in the scientific and industrial field but real-world problems require high-performance algorithms to optimize non-differentiable, non-convex, discontinuous functions. Different metaheuristics algorithms have been proposed to solve optimization problems but without strong assumptions about the objective function.","category":"page"},{"location":"","page":"Index","title":"Index","text":"This package implements state-of-the-art metaheuristics algorithms for global optimization. The package aims to provide easy-to-use (and fast) metaheuristics for numerical global optimization.","category":"page"},{"location":"#Installation","page":"Index","title":"Installation","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Open the Julia (Julia 1.1 or Later) REPL and press ] to open the Pkg prompt. To add this package, use the add command:","category":"page"},{"location":"","page":"Index","title":"Index","text":"pkg> add Metaheuristics","category":"page"},{"location":"","page":"Index","title":"Index","text":"Or, equivalently, via the Pkg API:","category":"page"},{"location":"","page":"Index","title":"Index","text":"julia> import Pkg; Pkg.add(\"Metaheuristics\")","category":"page"},{"location":"#Quick-Start","page":"Index","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Assume you want to solve the following minimization problem.","category":"page"},{"location":"","page":"Index","title":"Index","text":"(Image: Rastrigin Surface)","category":"page"},{"location":"","page":"Index","title":"Index","text":"Minimize:","category":"page"},{"location":"","page":"Index","title":"Index","text":"f(x) = 10D + sum_i=1^D  x_i^2 - 10cos(2pi x_i)","category":"page"},{"location":"","page":"Index","title":"Index","text":"where xin-5 5^D, i.e., -5 leq x_i leq 5 for i=1ldotsD. D is the dimension number, assume D=10.","category":"page"},{"location":"#Solution","page":"Index","title":"Solution","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Firstly, import the Metaheuristics package:","category":"page"},{"location":"","page":"Index","title":"Index","text":"using Metaheuristics","category":"page"},{"location":"","page":"Index","title":"Index","text":"Code the objective function:","category":"page"},{"location":"","page":"Index","title":"Index","text":"f(x) = 10length(x) + sum( x.^2 - 10cos.(2π*x)  )\nnothing # hide","category":"page"},{"location":"","page":"Index","title":"Index","text":"Instantiate the bounds:","category":"page"},{"location":"","page":"Index","title":"Index","text":"D = 10\nbounds = boxconstraints(lb = -5ones(D), ub = 5ones(D))\nnothing # hide","category":"page"},{"location":"","page":"Index","title":"Index","text":"Also, bounds can be a 2times 10 Matrix where the first row corresponds to the lower bounds whilst the second row corresponds to the upper bounds.","category":"page"},{"location":"","page":"Index","title":"Index","text":"Approximate the optimum using the function optimize.","category":"page"},{"location":"","page":"Index","title":"Index","text":"import Random: seed! # hide\nseed!(50) # hide\nresult = optimize(f, bounds)","category":"page"},{"location":"","page":"Index","title":"Index","text":"Optimize returns a State datatype which contains some information about the approximation. For instance, you may use mainly two functions to obtain such an approximation.","category":"page"},{"location":"","page":"Index","title":"Index","text":"minimum(result)","category":"page"},{"location":"","page":"Index","title":"Index","text":"minimizer(result)","category":"page"},{"location":"#Contents","page":"Index","title":"Contents","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Pages = [\"examples.md\", \"algorithms.md\", \"problems.md\", \"indicators.md\", \"mcdm.md\", \"visualization.md\", \"api.md\"]\nDepth = 2","category":"page"},{"location":"#Related-packages","page":"Index","title":"Related packages","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Evolutionary.jl: Genetic algorithms, \"Evolution\" Strategies, among others.\nGeneticAlgorithms.jl: Genetic Algorithms\nBlackBoxOptim.jl: Optimizers for black-box optimization (no information about the objective function).\nNODAL.jl: Stochastic Local Search methods, such as Simulated Annealing and Tabu Search.\nOther Packages.","category":"page"},{"location":"#How-to-cite?","page":"Index","title":"How to cite?","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Please cite the package using the bibtex entry ","category":"page"},{"location":"","page":"Index","title":"Index","text":"@article{metaheuristics2022, \n  doi = {10.21105/joss.04723}, \n  url = {https://doi.org/10.21105/joss.04723}, \n  year = {2022}, \n  publisher = {The Open Journal}, \n  volume = {7}, \n  number = {78}, \n  pages = {4723}, \n  author = {Jesús-Adolfo Mejía-de-Dios and Efrén Mezura-Montes}, \n  title = {Metaheuristics: A Julia Package for Single- and Multi-Objective Optimization}, \n journal = {Journal of Open Source Software} }","category":"page"},{"location":"","page":"Index","title":"Index","text":"or the citation string ","category":"page"},{"location":"","page":"Index","title":"Index","text":"Mejía-de-Dios et al., (2022). Metaheuristics: A Julia Package for Single- and Multi-Objective Optimization. Journal of Open Source Software, 7(78), 4723, https://doi.org/10.21105/joss.04723","category":"page"},{"location":"","page":"Index","title":"Index","text":"in your scientific paper if you use Metaheristics.jl. ","category":"page"},{"location":"#Acknowledgments","page":"Index","title":"Acknowledgments","text":"","category":"section"},{"location":"","page":"Index","title":"Index","text":"Jesús Mejía acknowledges support from the Mexican Council for Science and Technology (CONACyT) through a scholarship to pursue graduate studies at the University of Veracruz, MEXICO. This allowed the development of Metaheuristics.jl from August 2018 to July 2022.","category":"page"}]
}
